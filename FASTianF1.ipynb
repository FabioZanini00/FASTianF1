{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd6904cd",
   "metadata": {},
   "source": [
    "## Populate FASTianF1 RDF database\n",
    "\n",
    "This notebook reports the main steps to download CSV files, process them and create an RDF dataset from them accordingly to an ontology.\n",
    "\n",
    "To measure execution time in Jupyter notebooks: <code>pip install ipython-autotime</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b01ea34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# required libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c483ac7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the required libraries\n",
    "from rdflib import Graph, Literal, RDF, URIRef, Namespace\n",
    "# rdflib knows about some namespaces, like FOAF\n",
    "from rdflib.namespace import FOAF, XSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c483db2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK DATE \n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5fa963",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5543c38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters and URLs\n",
    "path = str(Path(os.path.abspath(os.getcwd())).parent.absolute())\n",
    "\n",
    "circuitsUrl = path + '\\FASTianF1\\data\\DatasetF1\\circuits.csv'\n",
    "constructor_resultsUrl = path + '\\FASTianF1\\data\\DatasetF1\\constructor_results.csv'\n",
    "constructor_standingsUrl = path + '\\FASTianF1\\data\\DatasetF1\\constructor_standings.csv'\n",
    "constructorsUrl = path + '\\FASTianF1\\data\\DatasetF1\\constructors.csv'\n",
    "driver_standingsUrl = path + '\\FASTianF1\\data\\DatasetF1\\driver_standings.csv'\n",
    "driversUrl = path + '\\FASTianF1\\data\\DatasetF1\\drivers.csv'\n",
    "lap_timesUrl = path + '\\FASTianF1\\data\\DatasetF1\\lap_times.csv'\n",
    "pit_stopsUrl = path + '\\FASTianF1\\data\\DatasetF1\\pit_stops.csv'\n",
    "qualifyingUrl = path + '\\FASTianF1\\data\\DatasetF1\\qualifying.csv'\n",
    "racesUrl = path + '\\FASTianF1\\data\\DatasetF1\\\\races.csv'\n",
    "resultsUrl = path + '\\FASTianF1\\data\\DatasetF1\\\\results.csv'\n",
    "sprint_resultsUrl = path + '\\FASTianF1\\data\\DatasetF1\\sprint_results.csv'\n",
    "statusUrl = path + '\\FASTianF1\\data\\DatasetF1\\status.csv'\n",
    "ratingsUrl = path + '\\FASTianF1\\data\\DatasetF1\\\\ratings.csv'\n",
    "\n",
    "# country codes and nationalities conversion\n",
    "countriesURL = path + '\\FASTianF1\\data\\countryCodes\\wikipedia-iso-country-codes.csv'\n",
    "nationalitiesURL = path + '\\FASTianF1\\data\\countryCodes\\\\nationalities.csv'\n",
    "\n",
    "# saving folder\n",
    "savePath =  path + '\\FASTianF1\\data\\\\rdf\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806e71b5",
   "metadata": {},
   "source": [
    "# Namespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf08a3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the country and the movie ontology namespaces not known by RDFlib\n",
    "CNS = Namespace(\"http://eulersharp.sourceforge.net/2003/03swap/countries#\")\n",
    "FO = Namespace(\"https://www.dei.unipd.it/db2/groupProject/FASTianF1#\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7880d54",
   "metadata": {},
   "source": [
    "# Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdf231c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the country codes\n",
    "# we need to convert NaN values to something else otherwise NA strings are converted to NaN -> problem with Namibia\n",
    "countries = pd.read_csv(countriesURL, sep=',', index_col='English short name lower case', keep_default_na=False, na_values=['_'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b8b325",
   "metadata": {},
   "source": [
    "# Drivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93a236e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "drivers = pd.read_csv(driversUrl, sep=',', index_col='driverId')\n",
    "nationalities = pd.read_csv(nationalitiesURL, sep=',', index_col='num_code')\n",
    "# cast year to int. If type(year) = str --> Literal= year-01-01\n",
    "# movies.astype({'year': 'int32'}).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "965c1b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"fo\", FO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "077e008d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 641 ms\n",
      "Wall time: 644 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the drivers dataframe\n",
    "for index, row in drivers.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + \"driver\" + the driver id as URI\n",
    "    Driver = URIRef(FO[\"driver\"+str(index)])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Driver, RDF.type, FO.Driver))\n",
    "    g.add((Driver, FO['hasDriverRef'], Literal(row['driverRef'], datatype=XSD.string)))\n",
    "    if(str(row['number']) != '\\\\N'):\n",
    "        g.add((Driver, FO['hasDriverNumber'], Literal(row['number'], datatype=XSD.integer)))\n",
    "    if(str(row['code']) != '\\\\N'):\n",
    "        g.add((Driver, FO['hasCode'], Literal(row['code'], datatype=XSD.string)))\n",
    "    g.add((Driver, FO['hasForename'], Literal(row['forename'], datatype=XSD.string)))\n",
    "    g.add((Driver, FO['hasSurname'], Literal(row['surname'], datatype=XSD.string)))\n",
    "    g.add((Driver, FO['hasURL'], Literal(row['url'], datatype=XSD.string)))\n",
    "    \n",
    "    #Check that the date has the year-month-day format otherwise print error\n",
    "    try:\n",
    "        datetime.datetime.strptime(str(row['dob']), '%Y-%m-%d')\n",
    "        g.add((Driver, FO['hasDateOfBirth'], Literal(row['dob'], datatype=XSD.date)))\n",
    "    except ValueError:\n",
    "        print(\"Incorrect date format\")\n",
    "\n",
    "    ## handle nationality\n",
    "    # there can be more than one nationality per driver\n",
    "    for nationality in str(row['nationality']).split('-'):\n",
    "        nationalityName = nationality.strip()\n",
    "        # check if the nationality exists in the nationalities dataframe\n",
    "        # str.contains() returns an array of booleans, thus we need to use the any() method\n",
    "        if((nationalities['nationality'].str.contains(nationalityName, case=False)).any() == True):\n",
    "            #get the country code, convert to string and get the lower case to match the country codes in the ontology\n",
    "            #There are multiple countries that correspond to American nationality, then the code for Americans is manually set to \"us\"\n",
    "            if(nationalityName != \"American\"):\n",
    "                code = str(nationalities[nationalities['nationality'].str.contains(nationalityName, case=False)]['alpha_2_code'].values[0]).lower()\n",
    "            else:\n",
    "                code = \"us\"\n",
    "            # create the RDF node for Country\n",
    "            Country = URIRef(CNS[code])\n",
    "            # add the edge connecting the Driver and the Country \n",
    "            g.add((Driver, FO['hasNation'], Country))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ff9af26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 188 ms\n",
      "Wall time: 171 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'drivers.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa815a7",
   "metadata": {},
   "source": [
    "# Circuits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6e32881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "circuits = pd.read_csv(circuitsUrl, sep=',', index_col='circuitId')\n",
    "# cast year to int. If type(year) = str --> Literal= year-01-01\n",
    "# movies.astype({'year': 'int32'}).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "026c7a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"fo\", FO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d99e3dd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 46.9 ms\n",
      "Wall time: 37.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "substitutions = {'ó': 'o', 'ü': 'u', 'ã': 'a', ' ': ''}\n",
    "substitutions2 = {'UAE': 'United Arab Emirates', 'USA': 'United States', 'UK': 'United Kingdom', 'Russia': 'Russian Federation', 'Korea': 'Korea, Republic of'}\n",
    "\n",
    "#iterate over the circuits dataframe\n",
    "for index, row in circuits.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + \"circuits\" + the circuit id as URI\n",
    "    Circuit = URIRef(FO[\"circuit\"+str(index)])\n",
    "    loc = str(row['location'])\n",
    "    # substitution of special characters with standard characters\n",
    "    # special characters are not allowed in URIs\n",
    "    for old, new in substitutions.items():\n",
    "        loc = loc.replace(old, new)\n",
    "    # create the RDF node for location\n",
    "    Location = URIRef(FO[\"location\"+loc])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Location, RDF.type, FO.Location))\n",
    "    g.add((Circuit, RDF.type, FO.Circuit))\n",
    "    g.add((Circuit, FO['hasCircuitRef'], Literal(row['circuitRef'], datatype=XSD.string)))\n",
    "    g.add((Circuit, FO['hasName'], Literal(row['name'], datatype=XSD.string)))\n",
    "    g.add((Circuit, FO['hasLat'], Literal(row['lat'], datatype=XSD.float)))\n",
    "    g.add((Circuit, FO['hasLng'], Literal(row['lng'], datatype=XSD.float)))\n",
    "    if(str(row['alt']) != '\\\\N'):\n",
    "        g.add((Circuit, FO['hasAlt'], Literal(row['alt'], datatype=XSD.float)))\n",
    "    g.add((Circuit, FO['hasURL'], Literal(row['url'], datatype=XSD.string)))\n",
    "    # add the edge connecting the Circuit and the Location \n",
    "    g.add((Circuit, FO['hasLocation'], Location))\n",
    "\n",
    "    ## handle country\n",
    "    countryName = str(row['country'])\n",
    "    # substitution of abbreviations in country names for full names\n",
    "    for old, new in substitutions2.items():\n",
    "        countryName = countryName.replace(old, new)\n",
    "    # check if the country exists\n",
    "    # str.contains() returns an array of booleans, thus we need to use the any() method\n",
    "    if((countries.index == countryName).any() == True):\n",
    "        #get the country code, convert to string and get the lower case to match the country codes in the ontology \n",
    "        code = str(countries[countries.index == countryName]['Alpha-2 code'][0]).lower()\n",
    "        # create the RDF node for Country\n",
    "        Country = URIRef(CNS[code])\n",
    "        # add the edge connecting the Location and the Country \n",
    "        g.add((Location, FO['hasCountry'], Country))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1e8fedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 46.9 ms\n",
      "Wall time: 21.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'circuits.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab133277",
   "metadata": {},
   "source": [
    "# Constructors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6908695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "constructors = pd.read_csv(constructorsUrl, sep=',', index_col='constructorId')\n",
    "# cast year to int. If type(year) = str --> Literal= year-01-01\n",
    "# movies.astype({'year': 'int32'}).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63c69926",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"fo\", FO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8612dfde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 141 ms\n",
      "Wall time: 133 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the constructors dataframe\n",
    "for index, row in constructors.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + \"constructor\" + the constructor id as URI\n",
    "    Constructor = URIRef(FO[\"constructor\"+str(index)])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Constructor, RDF.type, FO.Constructor))\n",
    "    g.add((Constructor, FO['hasConstructorRef'], Literal(row['constructorRef'], datatype=XSD.string)))\n",
    "    g.add((Constructor, FO['hasName'], Literal(row['name'], datatype=XSD.string)))\n",
    "    g.add((Constructor, FO['hasURL'], Literal(row['url'], datatype=XSD.string)))\n",
    "\n",
    "    ## handle nationality\n",
    "    #there can be more than one nationality per constructor\n",
    "    for nationality in str(row['nationality']).split('-'):\n",
    "        nationalityName = nationality.strip()\n",
    "        # check if the nationality exists\n",
    "        # str.contains() returns an array of booleans, thus we need to use the any() method\n",
    "        if((nationalities['nationality'].str.contains(nationalityName, case=False)).any() == True):\n",
    "            #get the country code, convert to string and get the lower case to match the country codes in the ontology \n",
    "            #There are multiple countries that correspond to American nationality, then the code for Americans is manually set to \"us\"\n",
    "            if(nationalityName != \"American\"):\n",
    "                code = str(nationalities[nationalities['nationality'].str.contains(nationalityName, case=False)]['alpha_2_code'].values[0]).lower()\n",
    "            else:\n",
    "                code = \"us\"\n",
    "            # create the RDF node for country\n",
    "            Country = URIRef(CNS[code])\n",
    "            # add the edge connecting the Constructor and the Country \n",
    "            g.add((Constructor, FO['hasNation'], Country))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59913aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 31.2 ms\n",
      "Wall time: 28.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'constructors.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577738ca",
   "metadata": {},
   "source": [
    "# Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29c60ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "status = pd.read_csv(statusUrl, sep=',', index_col='statusId')\n",
    "# cast year to int. If type(year) = str --> Literal= year-01-01\n",
    "# movies.astype({'year': 'int32'}).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74ff8b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"fo\", FO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c386e7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 12 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the status dataframe\n",
    "for index, row in status.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + \"status\" + the status id as URI\n",
    "    Status = URIRef(FO[\"status\"+str(index)])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Status, RDF.type, FO.Status))\n",
    "    g.add((Status, FO['hasName'], Literal(row['status'], datatype=XSD.string)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34df99ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 10.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'status.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dcd70b",
   "metadata": {},
   "source": [
    "# Race (and season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67e22c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "races = pd.read_csv(racesUrl, sep=',', index_col='raceId')\n",
    "# cast year to int. If type(year) = str --> Literal= year-01-01\n",
    "# movies.astype({'year': 'int32'}).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "687a35ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"fo\", FO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "340c39af",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 297 ms\n",
      "Wall time: 307 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the races dataframe\n",
    "for index, row in races.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + \"race\" + the race id as URI\n",
    "    Race = URIRef(FO[\"race\"+str(index)])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Race, RDF.type, FO.Race))\n",
    "    g.add((Race, FO['hasRound'], Literal(row['round'], datatype=XSD.integer)))\n",
    "    g.add((Race, FO['hasName'], Literal(row['name'], datatype=XSD.string)))\n",
    "    g.add((Race, FO['hasDate'], Literal(row['date'], datatype=XSD.date)))\n",
    "    if(str(row['time']) != '\\\\N'):\n",
    "        g.add((Race, FO['hasRaceTime'], Literal(row['time'], datatype=XSD.time)))\n",
    "    g.add((Race, FO['hasURL'], Literal(row['url'], datatype=XSD.string)))\n",
    "    \n",
    "    if(str(row['fp1_date']) != '\\\\N'):\n",
    "        g.add((Race, FO['hasFp1Date'], Literal(row['fp1_date'], datatype=XSD.date)))\n",
    "    if(str(row['fp1_time']) != '\\\\N'):\n",
    "        g.add((Race, FO['hasFp1Time'], Literal(row['fp1_time'], datatype=XSD.time)))\n",
    "    if(str(row['fp2_date']) != '\\\\N'):\n",
    "        g.add((Race, FO['hasFp2Date'], Literal(row['fp2_date'], datatype=XSD.date)))\n",
    "    if(str(row['fp2_time']) != '\\\\N'):\n",
    "        g.add((Race, FO['hasFp2Time'], Literal(row['fp2_time'], datatype=XSD.time)))\n",
    "    if(str(row['fp3_date']) != '\\\\N'):\n",
    "        g.add((Race, FO['hasFp3Date'], Literal(row['fp3_date'], datatype=XSD.date)))\n",
    "    if(str(row['fp3_time']) != '\\\\N'):\n",
    "        g.add((Race, FO['hasFp3Time'], Literal(row['fp3_time'], datatype=XSD.time)))\n",
    "    \n",
    "    # create the RDF node for circuit\n",
    "    Circuit = URIRef(FO[\"circuit\"+str(row['circuitId'])])\n",
    "    # add the edge connecting the Race and the Circuit \n",
    "    g.add((Race, FO['hasCircuit'], Circuit))\n",
    "    \n",
    "    # create the RDF node for season\n",
    "    Season = URIRef(FO[\"season\"+str(row['year'])])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Season, RDF.type, FO.Season))\n",
    "    g.add((Season, FO['hasYear'], Literal(row['year'], datatype=XSD.integer)))\n",
    "    g.add((Season, FO['hasURL'], Literal(row['url'], datatype=XSD.string)))\n",
    "    # add the edge connecting the Race and the Season \n",
    "    g.add((Race, FO['inSeason'], Season))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07d2285d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 281 ms\n",
      "Wall time: 278 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'race.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f27d02",
   "metadata": {},
   "source": [
    "# Race (partecipations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e5804573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "r_partecipations = pd.read_csv(resultsUrl, sep=',', index_col='resultId')\n",
    "driver_standings = pd.read_csv(driver_standingsUrl, sep=',', index_col=\"driverStandingsId\")\n",
    "constructor_standings = pd.read_csv(constructor_standingsUrl, sep=',', index_col=\"constructorStandingsId\")\n",
    "constructor_results = pd.read_csv(constructor_resultsUrl, sep=',', index_col=\"constructorResultsId\")\n",
    "# join is a dataframe containing the outer join of participation results (r_partecipations) \n",
    "# and driver results (driver_standings)\n",
    "# raceId and driverId are used as join keys\n",
    "join = r_partecipations.merge(driver_standings, how='outer', on=['raceId','driverId'], suffixes=('', 'Total')).fillna('\\\\N')\n",
    "# cast year to int. If type(year) = str --> Literal= year-01-01\n",
    "# movies.astype({'year': 'int32'}).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6cf33c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"fo\", FO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "85ae2206",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0 POSITION--> 1 FIRST TIME--> 1:34:50.616\n",
      "1\n",
      "START FIRST\n",
      "1:34:50.616000\n",
      "START DELTA\n",
      "1 POSITION--> 2 FIRST TIME--> 1900-01-01 01:34:50.616000 DELTA--> 0:00:05.478000 NEW TIME--> 01:34:56.094\n",
      "2\n",
      "START FIRST\n",
      "1:34:50.616000\n",
      "START DELTA\n",
      "2 POSITION--> 3 FIRST TIME--> 1900-01-01 01:34:50.616000 DELTA--> 0:00:08.163000 NEW TIME--> 01:34:58.779\n",
      "3\n",
      "START FIRST\n",
      "1:34:50.616000\n",
      "START DELTA\n",
      "3 POSITION--> 4 FIRST TIME--> 1900-01-01 01:34:50.616000 DELTA--> 0:00:17.181000 NEW TIME--> 01:35:07.797\n",
      "4\n",
      "START FIRST\n",
      "1:34:50.616000\n",
      "START DELTA\n",
      "4 POSITION--> 5 FIRST TIME--> 1900-01-01 01:34:50.616000 DELTA--> 0:00:18.014000 NEW TIME--> 01:35:08.630\n",
      "22\n",
      "23\n",
      "START FIRST\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:2606\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:2630\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:31\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:2419\u001b[0m, in \u001b[0;36m_AtIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2416\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid call for scalar access (getting)!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mloc[key]\n\u001b[1;32m-> 2419\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(key)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:2371\u001b[0m, in \u001b[0;36m_ScalarAccessIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2368\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid call for scalar access (getting)!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2370\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_key(key)\n\u001b[1;32m-> 2371\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_value(\u001b[38;5;241m*\u001b[39mkey, takeable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_takeable)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3877\u001b[0m, in \u001b[0;36mDataFrame._get_value\u001b[1;34m(self, index, col, takeable)\u001b[0m\n\u001b[0;32m   3871\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_engine\n\u001b[0;32m   3873\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, MultiIndex):\n\u001b[0;32m   3874\u001b[0m     \u001b[38;5;66;03m# CategoricalIndex: Trying to use the engine fastpath may give incorrect\u001b[39;00m\n\u001b[0;32m   3875\u001b[0m     \u001b[38;5;66;03m#  results if our categories are integers that dont match our codes\u001b[39;00m\n\u001b[0;32m   3876\u001b[0m     \u001b[38;5;66;03m# IntervalIndex: IntervalTree has no get_loc\u001b[39;00m\n\u001b[1;32m-> 3877\u001b[0m     row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mget_loc(index)\n\u001b[0;32m   3878\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m series\u001b[38;5;241m.\u001b[39m_values[row]\n\u001b[0;32m   3880\u001b[0m \u001b[38;5;66;03m# For MultiIndex going through engine effectively restricts us to\u001b[39;00m\n\u001b[0;32m   3881\u001b[0m \u001b[38;5;66;03m#  same-length tuples; see test_get_set_value_no_partial_indexing\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the join dataframe\n",
    "for index, row in join.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + \"r_partecipation\" + the partecipation id as URI\n",
    "    R_partecipation = URIRef(FO[\"r_partecipation\"+str(index)])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((R_partecipation, RDF.type, FO.RacePartecipation))\n",
    "    if(str(row['number']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasCarNumber'], Literal(row['number'], datatype=XSD.integer)))\n",
    "    if(str(row['grid']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasStartingGridPosition'], Literal(int(row['grid']), datatype=XSD.integer)))\n",
    "    if(str(row['position']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasPosition'], Literal(row['position'], datatype=XSD.integer)))\n",
    "    if(str(row['positionText']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasPositionText'], Literal(row['positionText'], datatype=XSD.string)))\n",
    "    if(str(row['positionOrder']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasPositionOrder'], Literal(int(row['positionOrder']), datatype=XSD.integer)))\n",
    "    if(str(row['points']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasPoints'], Literal(int(row['points']), datatype=XSD.integer)))\n",
    "    if(str(row['laps']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasLaps'], Literal(int(row['laps']), datatype=XSD.integer)))\n",
    "    if(str(row['time']) != '\\\\N'):\n",
    "        #g.add((R_partecipation, FO['hasResultTime'], Literal(row['time'], datatype=XSD.string)))\n",
    "        print(index)\n",
    "        if row['time'][0] == \"+\":\n",
    "            #FIRST_TIME RETRIEVAL\n",
    "            print(\"START FIRST\")\n",
    "            tmp = join[(join['raceId'] == row['raceId']) & (join['positionText'] == \"1\")]\n",
    "            print(\"Error\") if (tmp.shape[0]!=1) else None\n",
    "            first_time = datetime.datetime.strptime(str(tmp.at[0,'time'])+\"000\", \"%H:%M:%S.%f\")\n",
    "            #DELTA RETRIEVAL\n",
    "            print(\"START DELTA\")\n",
    "            time = str(row['time']).strip()[1:]\n",
    "            time_distance = \"00:00:00.000\"[0:12-len(time)] + time\n",
    "            (h, m, s) = time_distance.split('.')[0].split(':')\n",
    "            d = datetime.timedelta(hours=int(h), minutes=int(m), seconds=int(s), milliseconds=int(time_distance.split('.')[1]))\n",
    "            #FIRST_TIME + DELTA\n",
    "            new_time = first_time + d\n",
    "            g.add((R_partecipation, FO['hasResultTime'], Literal(new_time.strftime(\"%H:%M:%S.%f\")[:12], datatype=XSD.time)))\n",
    "            if(str(row['raceId']) == '18'):\n",
    "                print(index,\"POSITION-->\",str(row['positionText']),\"FIRST TIME-->\",first_time,\"DELTA-->\",d,\"NEW TIME-->\",\n",
    "                      new_time.strftime(\"%H:%M:%S.%f\")[:12])\n",
    "        else:\n",
    "            g.add((R_partecipation, FO['hasResultTime'], Literal(\"00:00:00.000\"[0:12-len(str(row['time']))] + \n",
    "                                                                 str(row['time']), datatype=XSD.time)))\n",
    "            if(str(row['raceId']) == '18'):\n",
    "                print(index,\"POSITION-->\",str(row['positionText']),\"FIRST TIME-->\",str(row['time']))\n",
    "    if(str(row['milliseconds']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasMillisecondsResultTime'], Literal(row['milliseconds'], datatype=XSD.integer)))\n",
    "    if(str(row['fastestLap']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasFastestLap'], Literal(row['fastestLap'], datatype=XSD.integer)))\n",
    "    if(str(row['rank']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasFastestLapRank'], Literal(row['rank'], datatype=XSD.integer)))\n",
    "    if(str(row['fastestLapTime']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasFastestLapTime'], Literal(\"00:00:00.000\"[0:12-len(str(row['fastestLapTime']))] + \n",
    "                                                                 str(row['fastestLapTime']), datatype=XSD.time)))\n",
    "    if(str(row['fastestLapSpeed']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasFastestLapSpeed'], Literal(row['fastestLapSpeed'], datatype=XSD.string)))\n",
    "    if(str(row['pointsTotal']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasTotalPoints'], Literal(int(row['pointsTotal']), datatype=XSD.integer)))\n",
    "    if(str(row['positionTotal']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasTotalPosition'], Literal(int(row['positionTotal']), datatype=XSD.integer)))\n",
    "    if(str(row['positionTextTotal']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasTotalPositionText'], Literal(row['positionTextTotal'], datatype=XSD.string)))\n",
    "    if(str(row['wins']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasDriversWins'], Literal(int(row['wins']), datatype=XSD.integer)))\n",
    "    \n",
    "    # create the RDF node for driver\n",
    "    Driver = URIRef(FO[\"driver\"+str(row['driverId'])])\n",
    "    # add the edge connecting the Partecipation and the Driver \n",
    "    g.add((R_partecipation, FO['hasDriver'], Driver))\n",
    "    \n",
    "    if(str(row['constructorId']) != '\\\\N'):\n",
    "        # create the RDF node for constructor\n",
    "        Constructor = URIRef(FO[\"constructor\"+str(int(row['constructorId']))])\n",
    "        # add the edge connecting the Partecipation and the Constructor \n",
    "        g.add((R_partecipation, FO['hasConstructor'], Constructor))\n",
    "    \n",
    "    # create the RDF node for race\n",
    "    Race = URIRef(FO[\"race\"+str(row['raceId'])])\n",
    "    # add the edge connecting the Partecipation and the Race \n",
    "    g.add((R_partecipation, FO['partecipatedInRace'], Race))\n",
    "    \n",
    "    if(str(row['statusId']) != '\\\\N'):\n",
    "        # create the RDF node for status\n",
    "        Status = URIRef(FO[\"status\"+str(int(row['statusId']))])\n",
    "        # add the edge connecting the Partecipation and the Status \n",
    "        g.add((R_partecipation, FO['hasStatus'], Status))\n",
    "\n",
    "#iterate over the constructor_standings dataframe\n",
    "for index, row in constructor_standings.iterrows():\n",
    "    # Get the rows of the join dataframe with the raceId and constructorId values matching those in the current row.\n",
    "    tmp = join[(join['raceId'] == row['raceId']) & (join['constructorId'] == row['constructorId'])]\n",
    "    #iterate over the rows found\n",
    "    for index2, row2 in tmp.iterrows():\n",
    "        # create the RDF node for racePartecipation\n",
    "        R_partecipation = URIRef(FO[\"r_partecipation\"+str(index2)])\n",
    "        # Add triples using store's add() method.\n",
    "        g.add((R_partecipation, FO['hasConstructorTotalPoints'], Literal(int(row['points']), datatype=XSD.integer)))\n",
    "        g.add((R_partecipation, FO['hasConstructorTotalPosition'], Literal(row['position'], datatype=XSD.integer)))\n",
    "        g.add((R_partecipation, FO['hasConstructorTotalPositionText'], Literal(row['positionText'], datatype=XSD.string)))\n",
    "        g.add((R_partecipation, FO['hasConstructorsWins'], Literal(int(row['wins']), datatype=XSD.integer)))\n",
    "\n",
    "#iterate over the constructor_results dataframe\n",
    "for index, row in constructor_results.iterrows():\n",
    "    # Get the rows of the join dataframe with the raceId and constructorId values matching those in the current row.\n",
    "    tmp = join[(join['raceId'] == row['raceId']) & (join['constructorId'] == row['constructorId'])]\n",
    "    #iterate over the rows found\n",
    "    for index2, row2 in tmp.iterrows():\n",
    "        # create the RDF node for racePartecipation\n",
    "        R_partecipation = URIRef(FO[\"r_partecipation\"+str(index2)])\n",
    "        # Add triples using store's add() method.\n",
    "        g.add((R_partecipation, FO['hasConstructorPoints'], Literal(int(row['points']), datatype=XSD.integer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "75c2c025",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 16.1 s\n",
      "Wall time: 16.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'race_partecipations.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2233e44",
   "metadata": {},
   "source": [
    "# Qualifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd044a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "q_partecipations = pd.read_csv(qualifyingUrl, sep=',', index_col='qualifyId')\n",
    "# cast year to int. If type(year) = str --> Literal= year-01-01\n",
    "# movies.astype({'year': 'int32'}).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b3a7a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"fo\", FO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "465d6aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 6.08 s\n",
      "Wall time: 6.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the q_partecipations dataframe\n",
    "for index, row in q_partecipations.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + \"q_partecipation\" + the qualifying partecipation id as URI\n",
    "    Q_partecipation = URIRef(FO[\"q_partecipation\"+str(index)])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Q_partecipation, RDF.type, FO.QualifPartecipation))\n",
    "    g.add((Q_partecipation, FO['hasCarNumber'], Literal(row['number'], datatype=XSD.integer)))\n",
    "    g.add((Q_partecipation, FO['hasPosition'], Literal(row['position'], datatype=XSD.integer)))\n",
    "    if(str(row['q1']) != '\\\\N'):\n",
    "        g.add((Q_partecipation, FO['hasQ1Time'], Literal(\"00:00:00.000\"[0:12-len(str(row['q1']))] + \n",
    "                                                         str(row['q1']), datatype=XSD.time)))\n",
    "    if(str(row['q2']) != '\\\\N'):\n",
    "        g.add((Q_partecipation, FO['hasQ2Time'], Literal(\"00:00:00.000\"[0:12-len(str(row['q2']))] + \n",
    "                                                         str(row['q2']), datatype=XSD.time)))\n",
    "    if(str(row['q3']) != '\\\\N'):\n",
    "        g.add((Q_partecipation, FO['hasQ3Time'], Literal(\"00:00:00.000\"[0:12-len(str(row['q3']))] + \n",
    "                                                         str(row['q3']), datatype=XSD.time)))\n",
    "\n",
    "    # create the RDF node for driver\n",
    "    Driver = URIRef(FO[\"driver\"+str(row['driverId'])])\n",
    "    # add the edge connecting the qualifyingPartecipation and the Driver \n",
    "    g.add((Q_partecipation, FO['hasDriver'], Driver))\n",
    "    \n",
    "    # create the RDF node for constructor\n",
    "    Constructor = URIRef(FO[\"constructor\"+str(row['constructorId'])])\n",
    "    # add the edge connecting the Partecipation and the Constructor \n",
    "    g.add((Q_partecipation, FO['hasConstructor'], Constructor))\n",
    "    \n",
    "    # create the RDF node for qualifying\n",
    "    Qualifying = URIRef(FO[\"qualifying\"+str(row['raceId'])])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Qualifying, RDF.type, FO.Qualifying))\n",
    "    # add the edge connecting the qualifyingPartecipation and the Qualifying \n",
    "    g.add((Q_partecipation, FO['partecipatedInQualif'], Qualifying))\n",
    "    \n",
    "    # create the RDF node for race\n",
    "    Race = URIRef(FO[\"race\"+str(row['raceId'])])\n",
    "    # add the edge connecting the Race and the Qualifying \n",
    "    g.add((Race, FO['hasA'], Qualifying))\n",
    "    \n",
    "    # Qualifying starting dates and times are stored in the races dataframe,\n",
    "    # then we retrieve them using the matching raceId\n",
    "    Q_date_time = races[races.index == row['raceId']]\n",
    "    if(str(Q_date_time['quali_date'].values[0]) != \"\\\\N\"):\n",
    "        g.add((Qualifying, FO['hasQualiDate'], Literal(Q_date_time['quali_date'].values[0], datatype=XSD.date)))\n",
    "    if(str(Q_date_time['quali_time'].values[0]) != \"\\\\N\"):\n",
    "        g.add((Qualifying, FO['hasQualiTime'], Literal(Q_date_time['quali_time'].values[0], datatype=XSD.time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "56abe1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 2.03 s\n",
      "Wall time: 2.04 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'qualifying.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4ff574",
   "metadata": {},
   "source": [
    "# Sprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9271534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "s_partecipations = pd.read_csv(sprint_resultsUrl, sep=',', index_col='resultId')\n",
    "# cast year to int. If type(year) = str --> Literal= year-01-01\n",
    "# movies.astype({'year': 'int32'}).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "af17e29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"fo\", FO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f6544469",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 141 ms\n",
      "Wall time: 154 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the s_partecipations dataframe\n",
    "for index, row in s_partecipations.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + \"s_partecipation\" + the sprint partecipation id as URI\n",
    "    S_partecipation = URIRef(FO[\"s_partecipation\"+str(index)])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((S_partecipation, RDF.type, FO.SprintPartecipation))\n",
    "    g.add((S_partecipation, FO['hasCarNumber'], Literal(row['number'], datatype=XSD.integer)))\n",
    "    g.add((S_partecipation, FO['hasStartingGridPosition'], Literal(row['grid'], datatype=XSD.integer)))\n",
    "    if(str(row['position']) != '\\\\N'):\n",
    "        g.add((S_partecipation, FO['hasPosition'], Literal(row['position'], datatype=XSD.integer)))\n",
    "    g.add((S_partecipation, FO['hasPositionText'], Literal(row['positionText'], datatype=XSD.string)))\n",
    "    g.add((S_partecipation, FO['hasPositionOrder'], Literal(row['positionOrder'], datatype=XSD.integer)))\n",
    "    g.add((S_partecipation, FO['hasPoints'], Literal(row['points'], datatype=XSD.integer)))\n",
    "    g.add((S_partecipation, FO['hasLaps'], Literal(row['laps'], datatype=XSD.integer)))\n",
    "    if(str(row['time']) != '\\\\N'):\n",
    "        g.add((S_partecipation, FO['hasResultTime'], Literal(row['time'], datatype=XSD.string)))\n",
    "    if(str(row['milliseconds']) != '\\\\N'):\n",
    "        g.add((S_partecipation, FO['hasMillisecondsResultTime'], Literal(row['milliseconds'], datatype=XSD.integer)))\n",
    "    if(str(row['fastestLap']) != '\\\\N'):\n",
    "        g.add((S_partecipation, FO['hasFastestLap'], Literal(row['fastestLap'], datatype=XSD.integer)))\n",
    "    if(str(row['fastestLapTime']) != '\\\\N'):\n",
    "        g.add((S_partecipation, FO['hasFastestLapTime'], Literal(\"00:00:00.000\"[0:12-len(str(row['fastestLapTime']))] + \n",
    "                                                                 str(row['fastestLapTime']), datatype=XSD.time)))\n",
    "    \n",
    "    # create the RDF node for driver\n",
    "    Driver = URIRef(FO[\"driver\"+str(row['driverId'])])\n",
    "    # add the edge connecting the sprintPartecipation and the Driver\n",
    "    g.add((S_partecipation, FO['hasDriver'], Driver))\n",
    "    \n",
    "    # create the RDF node for constructor\n",
    "    Constructor = URIRef(FO[\"constructor\"+str(row['constructorId'])])\n",
    "    # add the edge connecting the sprintPartecipation and the Constructor\n",
    "    g.add((S_partecipation, FO['hasConstructor'], Constructor))\n",
    "    \n",
    "    # create the RDF node for sprint\n",
    "    Sprint = URIRef(FO[\"sprint\"+str(row['raceId'])])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Sprint, RDF.type, FO.Sprint))\n",
    "    # add the edge connecting the sprintPartecipation and the Sprint \n",
    "    g.add((S_partecipation, FO['partecipatedInSprint'], Sprint))\n",
    "    \n",
    "    # create the RDF node for race\n",
    "    Race = URIRef(FO[\"race\"+str(row['raceId'])])\n",
    "    # add the edge connecting the Race and the Sprint \n",
    "    g.add((Race, FO['hasA'], Sprint))\n",
    "    \n",
    "    # create the RDF node for status\n",
    "    Status = URIRef(FO[\"status\"+str(row['statusId'])])\n",
    "    # add the edge connecting the sprintPartecipation and the Sprint \n",
    "    g.add((S_partecipation, FO['hasStatus'], Status))\n",
    "    \n",
    "    # Sprint starting dates and times are stored in the races dataframe,\n",
    "    # then we retrieve them using the matching raceId\n",
    "    S_date_time = races[races.index == row['raceId']]\n",
    "    if(str(S_date_time['sprint_date'].values[0]) != \"\\\\N\"):\n",
    "        g.add((Sprint, FO['hasSprintDate'], Literal(S_date_time['sprint_date'].values[0], datatype=XSD.date)))\n",
    "    if(str(S_date_time['sprint_time'].values[0]) != \"\\\\N\"):\n",
    "        g.add((Sprint, FO['hasSprintTime'], Literal(S_date_time['sprint_time'].values[0], datatype=XSD.time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "111ccaf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 78.1 ms\n",
      "Wall time: 86.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'sprint.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544125de",
   "metadata": {},
   "source": [
    "# Laps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c504383b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "laps = pd.read_csv(lap_timesUrl, sep=',')\n",
    "# cast year to int. If type(year) = str --> Literal= year-01-01\n",
    "# movies.astype({'year': 'int32'}).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "283ca1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"fo\", FO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bbb691b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 7min 53s\n",
      "Wall time: 7min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "# id for the Lap URI\n",
    "id = 1\n",
    "#iterate over the laps dataframe\n",
    "for index, row in laps.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + \"lap\" + the lap id as URI\n",
    "    Lap = URIRef(FO[\"lap\"+str(id)])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Lap, RDF.type, FO.Lap))\n",
    "    g.add((Lap, FO['hasLapNumber'], Literal(row['lap'], datatype=XSD.integer)))\n",
    "    g.add((Lap, FO['hasLapPosition'], Literal(row['position'], datatype=XSD.integer)))\n",
    "    g.add((Lap, FO['hasLapTime'], Literal(\"00:00:00.000\"[0:12-len(str(row['time']))] + \n",
    "                                          str(row['time']), datatype=XSD.time)))\n",
    "    g.add((Lap, FO['hasMillisecondsTime'], Literal(row['milliseconds'], datatype=XSD.integer)))\n",
    "    \n",
    "    # Get the rows of the join dataframe with the raceId and driverId values matching those in the current row.\n",
    "    tmp = join[(join['raceId'] == row['raceId']) & (join['driverId'] == row['driverId'])]\n",
    "    #iterate over the rows found\n",
    "    for index2, row2 in tmp.iterrows():\n",
    "        # create the RDF node for racePartecipation\n",
    "        R_partecipation = URIRef(FO[\"r_partecipation\"+str(index2)])\n",
    "        # add the edge connecting the racePartecipation and the Lap \n",
    "        g.add((R_partecipation, FO['hasLap'], Lap))\n",
    "    \n",
    "    id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4be10ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 1min 30s\n",
      "Wall time: 1min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'laps.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52bc8b3",
   "metadata": {},
   "source": [
    "# Pit stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "190fb428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "pit_stops = pd.read_csv(pit_stopsUrl, sep=',')\n",
    "# cast year to int. If type(year) = str --> Literal= year-01-01\n",
    "# movies.astype({'year': 'int32'}).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4c2ef890",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"fo\", FO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2d447acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 24.6 s\n",
      "Wall time: 24.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "# id for the PitStop URI\n",
    "id = 1\n",
    "#iterate over the pit_stops dataframe\n",
    "for index, row in pit_stops.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + \"stop\" + the movie id as URI\n",
    "    PitStop = URIRef(FO[\"stop\"+str(id)])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((PitStop, RDF.type, FO.PitStop))\n",
    "    g.add((PitStop, FO['hasStopNumber'], Literal(row['stop'], datatype=XSD.integer)))\n",
    "    g.add((PitStop, FO['hasPitStopTimeOfDay'], Literal(str(row['time']), datatype=XSD.time)))\n",
    "    g.add((PitStop, FO['hasMillisecondsTime'], Literal(row['milliseconds'], datatype=XSD.integer)))\n",
    "    g.add((PitStop, FO['hasDuration'], Literal(\"00:00:00.000\"[0:12-len(str(row['duration']))] + \n",
    "                                               str(row['duration']), datatype=XSD.time)))\n",
    "    \n",
    "    # Get the rows of the laps dataframe with the raceId, driverId and lap values matching those in the current row.\n",
    "    tmp = laps[(laps['raceId'] == row['raceId']) & (laps['driverId'] == row['driverId']) & (laps['lap'] == row['lap'])]\n",
    "    #iterate over the rows found\n",
    "    for index2, row2 in tmp.iterrows():\n",
    "        # create the RDF node for lap\n",
    "        Lap = URIRef(FO[\"lap\"+str(index2)])\n",
    "        # add the edge connecting the PitStop and the Lap \n",
    "        g.add((PitStop, FO['hasPitStopLap'], Lap))\n",
    "        \n",
    "    # Get the rows of the join dataframe with the raceId and driverId values matching those in the current row.\n",
    "    tmp2 = join[(join['raceId'] == row['raceId']) & (join['driverId'] == row['driverId'])]\n",
    "    #iterate over the rows found\n",
    "    for index2, row2 in tmp2.iterrows():\n",
    "        # create the RDF node for racePartecipation\n",
    "        R_partecipation = URIRef(FO[\"r_partecipation\"+str(index2)])\n",
    "        # add the edge connecting the racePartecipation and the PitStop \n",
    "        g.add((R_partecipation, FO['hasPitStop'], PitStop))\n",
    "    \n",
    "    id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eaa6d059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 2.08 s\n",
      "Wall time: 2.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'stops.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fdf6c5",
   "metadata": {},
   "source": [
    "# Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8b46dbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "ratings = pd.read_csv(ratingsUrl, sep=',')\n",
    "# cast year to int. If type(year) = str --> Literal= year-01-01\n",
    "# movies.astype({'year': 'int32'}).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "feff869d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"fo\", FO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fad35fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 297 ms\n",
      "Wall time: 296 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "# id for the Rating URI\n",
    "id = 1\n",
    "#iterate over the ratings dataframe\n",
    "for index, row in ratings.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + \"rating\" + the movie id as URI\n",
    "    Rating = URIRef(FO[\"rating\"+str(id)])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Rating, RDF.type, FO.Rating))\n",
    "    g.add((Rating, FO['hasPeriod'], Literal(row['Period'], datatype=XSD.date)))\n",
    "    g.add((Rating, FO['hasRating'], Literal(row['Rating'], datatype=XSD.integer)))\n",
    "    g.add((Rating, FO['hasExperience'], Literal(row['Experience'], datatype=XSD.integer)))\n",
    "    g.add((Rating, FO['hasRaceCraft'], Literal(row['Race Craft'], datatype=XSD.integer)))\n",
    "    g.add((Rating, FO['hasAwareness'], Literal(row['Awareness'], datatype=XSD.integer)))\n",
    "    g.add((Rating, FO['hasPace'], Literal(row['Pace'], datatype=XSD.integer)))\n",
    "    g.add((Rating, FO['hasContractCost'], Literal(row['Contract Cost'], datatype=XSD.long)))\n",
    "    g.add((Rating, FO['hasSalary'], Literal(row['Salary'], datatype=XSD.long)))\n",
    "    g.add((Rating, FO['hasBuyout'], Literal(row['Buyout'], datatype=XSD.long)))\n",
    "    \n",
    "    # create the RDF node for season\n",
    "    Season = URIRef(FO[\"year\"+str(row['Year'])])\n",
    "    # add the edge connecting the Rating and the Season \n",
    "    g.add((Rating, FO['inSeason'], Season))\n",
    "    \n",
    "    # in ratings csv names are full-names, then they are splitted in forename and surname\n",
    "    name = str(row['Driver']).split(' ')\n",
    "    forename = name[0].strip()\n",
    "    surname = name[1].strip()\n",
    "    # Get the rows of the drivers dataframe with the forename value (rating csv) contained in forename column of the current row (driver csv).\n",
    "    subCsv = drivers[drivers['forename'].str.contains(forename, case=False)]\n",
    "    # If exists at least one row in subCsv that contain also the surname of the rating\n",
    "    if((subCsv['surname'].str.contains(surname, case=False)).any() == True):\n",
    "        # Get the rows of the drivers dataframe with the forename and surname values contained in the matching column of the current row.\n",
    "        dId = drivers[(drivers['forename'].str.contains(forename, case=False)) & (drivers['surname'].str.contains(surname, case=False))].index.values[0]\n",
    "        # create the RDF node for driver\n",
    "        Driver = URIRef(FO[\"driver\"+str(dId)])\n",
    "        # add the edge connecting the Rating and the Driver \n",
    "        g.add((Rating, FO['hasDriver'], Driver))\n",
    "        \n",
    "    id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5dc07d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 62.5 ms\n",
      "Wall time: 51.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'ratings.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c823e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
