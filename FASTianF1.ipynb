{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd6904cd",
   "metadata": {},
   "source": [
    "## Populate FASTianF1 RDF database\n",
    "\n",
    "This notebook reports the main steps to download CSV files, process them and create an RDF dataset from them accordingly to an ontology.\n",
    "\n",
    "To measure execution time in Jupyter notebooks: <code>pip install ipython-autotime</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b01ea34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# required libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c483ac7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the required libraries\n",
    "from rdflib import Graph, Literal, RDF, URIRef, Namespace\n",
    "# rdflib knows about some namespaces, like FOAF\n",
    "from rdflib.namespace import FOAF, XSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c483db2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK DATE \n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5fa963",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5543c38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters and URLs\n",
    "path = str(Path(os.path.abspath(os.getcwd())).parent.absolute())\n",
    "# print(path)\n",
    "circuitsUrl = path + '\\FASTianF1\\data\\DatasetF1\\circuits.csv'\n",
    "# print(circuitsUrl)\n",
    "constructor_resultsUrl = path + '\\FASTianF1\\data\\DatasetF1\\constructor_results.csv'\n",
    "constructor_standingsUrl = path + '\\FASTianF1\\data\\DatasetF1\\constructor_standings.csv'\n",
    "constructorsUrl = path + '\\FASTianF1\\data\\DatasetF1\\constructors.csv'\n",
    "driver_standingsUrl = path + '\\FASTianF1\\data\\DatasetF1\\driver_standings.csv'\n",
    "driversUrl = path + '\\FASTianF1\\data\\DatasetF1\\drivers.csv'\n",
    "lap_timesUrl = path + '\\FASTianF1\\data\\DatasetF1\\lap_times.csv'\n",
    "pit_stopsUrl = path + '\\FASTianF1\\data\\DatasetF1\\pit_stops.csv'\n",
    "qualifyingUrl = path + '\\FASTianF1\\data\\DatasetF1\\qualifying.csv'\n",
    "racesUrl = path + '\\FASTianF1\\data\\DatasetF1\\\\races.csv'\n",
    "resultsUrl = path + '\\FASTianF1\\data\\DatasetF1\\\\results.csv'\n",
    "sprint_resultsUrl = path + '\\FASTianF1\\data\\DatasetF1\\sprint_results.csv'\n",
    "statusUrl = path + '\\FASTianF1\\data\\DatasetF1\\status.csv'\n",
    "\n",
    "# country codes\n",
    "countriesURL = path + '\\FASTianF1\\data\\countryCodes\\wikipedia-iso-country-codes.csv'\n",
    "nationalitiesURL = path + '\\FASTianF1\\data\\countryCodes\\\\nationalities.csv'\n",
    "\n",
    "# saving folder\n",
    "savePath =  path + '\\FASTianF1\\data\\\\rdf\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806e71b5",
   "metadata": {},
   "source": [
    "# Namespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf08a3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the country and the movie ontology namespaces not known by RDFlib\n",
    "CNS = Namespace(\"http://eulersharp.sourceforge.net/2003/03swap/countries#\")\n",
    "FO = Namespace(\"https://www.dei.unipd.it/db2/groupProject/FASTianF1#\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7880d54",
   "metadata": {},
   "source": [
    "# Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdf231c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the country codes\n",
    "# we need to convert NaN values to something else otherwise NA strings are converted to NaN -> problem with Namibia\n",
    "countries = pd.read_csv(countriesURL, sep=',', index_col='English short name lower case', keep_default_na=False, na_values=['_'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b8b325",
   "metadata": {},
   "source": [
    "# Drivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93a236e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "drivers = pd.read_csv(driversUrl, sep=',', index_col='driverId')\n",
    "nationalities = pd.read_csv(nationalitiesURL, sep=',', index_col='num_code')\n",
    "# cast year to int. If type(year) = str --> Literal= year-01-01\n",
    "# movies.astype({'year': 'int32'}).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "965c1b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"fo\", FO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "077e008d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 594 ms\n",
      "Wall time: 839 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the movies dataframe\n",
    "for index, row in drivers.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the movie id as URI\n",
    "    Driver = URIRef(FO[\"driver\"+str(index)])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Driver, RDF.type, FO.Driver))\n",
    "    g.add((Driver, FO['hasDriverRef'], Literal(row['driverRef'], datatype=XSD.string)))\n",
    "    if(str(row['number']) != '\\\\N'):\n",
    "        g.add((Driver, FO['hasDriverNumber'], Literal(row['number'], datatype=XSD.integer)))\n",
    "    if(str(row['code']) != '\\\\N'):\n",
    "        g.add((Driver, FO['hasCode'], Literal(row['code'], datatype=XSD.string)))\n",
    "    g.add((Driver, FO['hasForename'], Literal(row['forename'], datatype=XSD.string)))\n",
    "    g.add((Driver, FO['hasSurname'], Literal(row['surname'], datatype=XSD.string)))\n",
    "    #g.add((Driver, FO['hasDateOfBirth'], Literal(row['dob'], datatype=XSD.string)))\n",
    "    g.add((Driver, FO['hasURL'], Literal(row['url'], datatype=XSD.string)))\n",
    "    \n",
    "    try:\n",
    "        datetime.datetime.strptime(str(row['dob']), '%Y-%m-%d')\n",
    "        g.add((Driver, FO['hasDateOfBirth'], Literal(row['dob'], datatype=XSD.date)))\n",
    "    except ValueError:\n",
    "        # probably it's the year alone\n",
    "        # check length\n",
    "        if (len(row['dob'])==4):\n",
    "            #it is a year\n",
    "            g.add((Driver, FO['hasDateOfBirth'], Literal(row['dob']+\"-01-01\", datatype=XSD.date)))\n",
    "\n",
    "    ## handle country\n",
    "    #there can be more than one country per movie\n",
    "    for c in str(row['nationality']).split('-'):\n",
    "        cName = c.strip()\n",
    "        # check if the country exists\n",
    "        # country.index == x returns an array of booleans, thus we need to use the any() method\n",
    "        if((nationalities['nationality'].str.contains(cName, case=False)).any() == True):\n",
    "            #get the country code, convert to string and get the lower case to match the country codes in the ontology \n",
    "            #print(str(nationalities[nationalities['nationality'].str.contains(cName, case=False)]['alpha_2_code'].values[0]))\n",
    "            if(cName != \"American\"):\n",
    "                code = str(nationalities[nationalities['nationality'].str.contains(cName, case=False)]['alpha_2_code'].values[0]).lower()\n",
    "            else:\n",
    "                code = \"us\"\n",
    "            # create the RDF node\n",
    "            Country = URIRef(CNS[code])\n",
    "            # add the edge connecting the Movie and the Country \n",
    "            g.add((Driver, FO['hasNation'], Country))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ff9af26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 156 ms\n",
      "Wall time: 237 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'drivers.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa815a7",
   "metadata": {},
   "source": [
    "# Circuits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6e32881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "circuits = pd.read_csv(circuitsUrl, sep=',', index_col='circuitId')\n",
    "# cast year to int. If type(year) = str --> Literal= year-01-01\n",
    "# movies.astype({'year': 'int32'}).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "026c7a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"fo\", FO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d99e3dd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 49.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the movies dataframe\n",
    "for index, row in circuits.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the movie id as URI\n",
    "    Circuit = URIRef(FO[\"circuit\"+str(index)])\n",
    "    Location = URIRef(FO[\"location\"+str(row['location']).replace(\" \", \"\")])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Circuit, RDF.type, FO.Circuit))\n",
    "    g.add((Circuit, FO['hasCircuitRef'], Literal(row['circuitRef'], datatype=XSD.string)))\n",
    "    g.add((Circuit, FO['hasName'], Literal(row['name'], datatype=XSD.string)))\n",
    "    g.add((Location, RDF.type, FO.Location))\n",
    "    g.add((Circuit, FO['hasLocation'], Location))\n",
    "    g.add((Circuit, FO['hasLat'], Literal(row['lat'], datatype=XSD.float)))\n",
    "    g.add((Circuit, FO['hasLng'], Literal(row['lng'], datatype=XSD.float)))\n",
    "    if(str(row['alt']) != '\\\\N'):\n",
    "        g.add((Circuit, FO['hasAlt'], Literal(row['alt'], datatype=XSD.float)))\n",
    "    g.add((Circuit, FO['hasURL'], Literal(row['url'], datatype=XSD.string)))\n",
    "\n",
    "    ## handle country\n",
    "    cName = str(row['country'])\n",
    "    # check if the country exists\n",
    "    # country.index == x returns an array of booleans, thus we need to use the any() method\n",
    "    if((countries.index == cName).any() == True):\n",
    "        #get the country code, convert to string and get the lower case to match the country codes in the ontology \n",
    "        code = str(countries[countries.index == cName]['Alpha-2 code'][0]).lower()\n",
    "        # create the RDF node\n",
    "        Country = URIRef(CNS[code])\n",
    "        # add the edge connecting the Movie and the Country \n",
    "        g.add((Location, FO['hasCountry'], Country))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1e8fedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 35.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'circuits.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab133277",
   "metadata": {},
   "source": [
    "# Constructors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6908695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "constructors = pd.read_csv(constructorsUrl, sep=',', index_col='constructorId')\n",
    "# cast year to int. If type(year) = str --> Literal= year-01-01\n",
    "# movies.astype({'year': 'int32'}).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63c69926",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"fo\", FO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8612dfde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 31.2 ms\n",
      "Wall time: 197 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the movies dataframe\n",
    "for index, row in constructors.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the movie id as URI\n",
    "    Constructor = URIRef(FO[\"constructor\"+str(index)])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Constructor, RDF.type, FO.Constructor))\n",
    "    g.add((Constructor, FO['hasConstructorRef'], Literal(row['constructorRef'], datatype=XSD.string)))\n",
    "    g.add((Constructor, FO['hasName'], Literal(row['name'], datatype=XSD.string)))\n",
    "    g.add((Constructor, FO['hasURL'], Literal(row['url'], datatype=XSD.string)))\n",
    "\n",
    "    ## handle country\n",
    "    #there can be more than one country per movie\n",
    "    for c in str(row['nationality']).split('-'):\n",
    "        cName = c.strip()\n",
    "        # check if the country exists\n",
    "        # country.index == x returns an array of booleans, thus we need to use the any() method\n",
    "        if((nationalities['nationality'].str.contains(cName, case=False)).any() == True):\n",
    "            #get the country code, convert to string and get the lower case to match the country codes in the ontology \n",
    "            #print(str(nationalities[nationalities['nationality'].str.contains(cName, case=False)]['alpha_2_code'].shape[0]))\n",
    "            if(cName != \"American\"):\n",
    "                code = str(nationalities[nationalities['nationality'].str.contains(cName, case=False)]['alpha_2_code'].values[0]).lower()\n",
    "            else:\n",
    "                code = \"us\"\n",
    "            # create the RDF node\n",
    "            Country = URIRef(CNS[code])\n",
    "            # add the edge connecting the Movie and the Country \n",
    "            g.add((Constructor, FO['hasNation'], Country))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59913aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 48.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'constructors.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577738ca",
   "metadata": {},
   "source": [
    "# Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29c60ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "status = pd.read_csv(statusUrl, sep=',', index_col='statusId')\n",
    "# cast year to int. If type(year) = str --> Literal= year-01-01\n",
    "# movies.astype({'year': 'int32'}).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74ff8b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"fo\", FO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c386e7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 16.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the movies dataframe\n",
    "for index, row in status.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the movie id as URI\n",
    "    Status = URIRef(FO[\"status\"+str(index)])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Status, RDF.type, FO.Status))\n",
    "    g.add((Status, FO['hasName'], Literal(row['status'], datatype=XSD.string)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34df99ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 16.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'status.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dcd70b",
   "metadata": {},
   "source": [
    "# Race (and season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67e22c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "races = pd.read_csv(racesUrl, sep=',', index_col='raceId')\n",
    "# cast year to int. If type(year) = str --> Literal= year-01-01\n",
    "# movies.astype({'year': 'int32'}).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "687a35ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"fo\", FO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "340c39af",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 281 ms\n",
      "Wall time: 373 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the movies dataframe\n",
    "for index, row in races.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the movie id as URI\n",
    "    Race = URIRef(FO[\"race\"+str(index)])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Race, RDF.type, FO.Race))\n",
    "    g.add((Race, FO['hasRound'], Literal(row['round'], datatype=XSD.integer)))\n",
    "    g.add((Race, FO['hasName'], Literal(row['name'], datatype=XSD.string)))\n",
    "    g.add((Race, FO['hasDate'], Literal(row['date'], datatype=XSD.date)))\n",
    "    if(str(row['time']) != '\\\\N'):\n",
    "        g.add((Race, FO['hasRaceTime'], Literal(row['time'], datatype=XSD.time)))\n",
    "    g.add((Race, FO['hasURL'], Literal(row['url'], datatype=XSD.string)))\n",
    "    \n",
    "    if(str(row['fp1_date']) != '\\\\N'):\n",
    "        g.add((Race, FO['hasFp1Date'], Literal(row['fp1_date'], datatype=XSD.date)))\n",
    "    if(str(row['fp1_time']) != '\\\\N'):\n",
    "        g.add((Race, FO['hasFp1Time'], Literal(row['fp1_time'], datatype=XSD.time)))\n",
    "    if(str(row['fp2_date']) != '\\\\N'):\n",
    "        g.add((Race, FO['hasFp2Date'], Literal(row['fp2_date'], datatype=XSD.date)))\n",
    "    if(str(row['fp2_time']) != '\\\\N'):\n",
    "        g.add((Race, FO['hasFp2Time'], Literal(row['fp2_time'], datatype=XSD.time)))\n",
    "    if(str(row['fp3_date']) != '\\\\N'):\n",
    "        g.add((Race, FO['hasFp3Date'], Literal(row['fp3_date'], datatype=XSD.date)))\n",
    "    if(str(row['fp3_time']) != '\\\\N'):\n",
    "        g.add((Race, FO['hasFp3Time'], Literal(row['fp3_time'], datatype=XSD.time)))\n",
    "    \n",
    "    Circuit = URIRef(FO[\"circuit\"+str(row['circuitId'])])\n",
    "    g.add((Race, FO['hasCircuit'], Circuit))\n",
    "    \n",
    "    Season = URIRef(FO[\"season\"+str(row['year'])])\n",
    "    g.add((Season, RDF.type, FO.Season))\n",
    "    g.add((Season, FO['hasYear'], Literal(row['year'], datatype=XSD.integer)))\n",
    "    g.add((Race, FO['inSeason'], Season))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07d2285d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 188 ms\n",
      "Wall time: 300 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'race.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f27d02",
   "metadata": {},
   "source": [
    "# Race (partecipations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e5804573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "r_partecipations = pd.read_csv(resultsUrl, sep=',', index_col='resultId')\n",
    "driver_standings = pd.read_csv(driver_standingsUrl, sep=',', index_col=\"driverStandingsId\")\n",
    "constructor_standings = pd.read_csv(constructor_standingsUrl, sep=',', index_col=\"constructorStandingsId\")\n",
    "constructor_results = pd.read_csv(constructor_resultsUrl, sep=',', index_col=\"constructorResultsId\")\n",
    "join = r_partecipations.merge(driver_standings, how='outer', on=['raceId','driverId'], suffixes=('', 'Total')).fillna('\\\\N')\n",
    "# cast year to int. If type(year) = str --> Literal= year-01-01\n",
    "# movies.astype({'year': 'int32'}).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6cf33c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"fo\", FO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "85ae2206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 10s\n",
      "Wall time: 1min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the movies dataframe\n",
    "for index, row in join.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the movie id as URI\n",
    "    R_partecipation = URIRef(FO[\"r_partecipation\"+str(index)])\n",
    "    #print(URIRef(str(index)))\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((R_partecipation, RDF.type, FO.RacePartecipation))\n",
    "    if(str(row['number']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasCarNumber'], Literal(row['number'], datatype=XSD.integer)))\n",
    "    if(str(row['grid']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasStartingGridPosition'], Literal(int(row['grid']), datatype=XSD.integer)))\n",
    "    if(str(row['position']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasPosition'], Literal(row['position'], datatype=XSD.integer)))\n",
    "    if(str(row['positionText']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasPositionText'], Literal(row['positionText'], datatype=XSD.string)))\n",
    "    if(str(row['positionOrder']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasPositionOrder'], Literal(int(row['positionOrder']), datatype=XSD.integer)))\n",
    "    if(str(row['points']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasPoints'], Literal(int(row['points']), datatype=XSD.integer)))\n",
    "    if(str(row['laps']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasLaps'], Literal(int(row['laps']), datatype=XSD.integer)))\n",
    "    if(str(row['time']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasResultTime'], Literal(row['time'], datatype=XSD.string)))\n",
    "    if(str(row['milliseconds']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasMillisecondsResultTime'], Literal(row['milliseconds'], datatype=XSD.integer)))\n",
    "    if(str(row['fastestLap']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasFastestLap'], Literal(row['fastestLap'], datatype=XSD.integer)))\n",
    "    if(str(row['rank']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasFastestLapRank'], Literal(row['rank'], datatype=XSD.integer)))\n",
    "    if(str(row['fastestLapTime']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasFastestLapTime'], Literal(\"00:0\" + str(row['fastestLapTime']), datatype=XSD.time)))\n",
    "    if(str(row['fastestLapSpeed']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasFastestLapSpeed'], Literal(row['fastestLapSpeed'], datatype=XSD.string)))\n",
    "    if(str(row['pointsTotal']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasTotalPoints'], Literal(int(row['pointsTotal']), datatype=XSD.integer)))\n",
    "    if(str(row['positionTotal']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasTotalPosition'], Literal(int(row['positionTotal']), datatype=XSD.integer)))\n",
    "    if(str(row['positionTextTotal']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasTotalPositionText'], Literal(row['positionTextTotal'], datatype=XSD.string)))\n",
    "    if(str(row['wins']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasDriversWins'], Literal(int(row['wins']), datatype=XSD.integer)))\n",
    "    \n",
    "    Driver = URIRef(FO[\"driver\"+str(row['driverId'])])\n",
    "    g.add((R_partecipation, FO['hasDriver'], Driver))\n",
    "    \n",
    "    if(str(row['constructorId']) != '\\\\N'):\n",
    "        Constructor = URIRef(FO[\"constructor\"+str(int(row['constructorId']))])\n",
    "        g.add((R_partecipation, FO['hasConstructor'], Constructor))\n",
    "    \n",
    "    Race = URIRef(FO[\"race\"+str(row['raceId'])])\n",
    "    g.add((R_partecipation, FO['partecipatedInRace'], Race))\n",
    "    \n",
    "    if(str(row['statusId']) != '\\\\N'):\n",
    "        Status = URIRef(FO[\"status\"+str(int(row['statusId']))])\n",
    "        g.add((R_partecipation, FO['hasStatus'], Status))\n",
    "\n",
    "for index, row in constructor_standings.iterrows():\n",
    "    tmp = join[(join['raceId'] == row['raceId']) & (join['constructorId'] == row['constructorId'])]\n",
    "    for index2, row2 in tmp.iterrows():\n",
    "        R_partecipation = URIRef(FO[\"r_partecipation\"+str(index2)])\n",
    "        g.add((R_partecipation, FO['hasConstructorTotalPoints'], Literal(int(row['points']), datatype=XSD.integer)))\n",
    "        g.add((R_partecipation, FO['hasConstructorTotalPosition'], Literal(row['position'], datatype=XSD.integer)))\n",
    "        g.add((R_partecipation, FO['hasConstructorTotalPositionText'], Literal(row['positionText'], datatype=XSD.string)))\n",
    "        g.add((R_partecipation, FO['hasConstructorsWins'], Literal(int(row['wins']), datatype=XSD.integer)))\n",
    "        \n",
    "for index, row in constructor_results.iterrows():\n",
    "    tmp = join[(join['raceId'] == row['raceId']) & (join['constructorId'] == row['constructorId'])]\n",
    "    for index2, row2 in tmp.iterrows():\n",
    "        R_partecipation = URIRef(FO[\"r_partecipation\"+str(index2)])\n",
    "        g.add((R_partecipation, FO['hasConstructorPoints'], Literal(int(row['points']), datatype=XSD.integer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "75c2c025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 15.5 s\n",
      "Wall time: 19.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'race_partecipations.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2233e44",
   "metadata": {},
   "source": [
    "# Qualifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd044a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "q_partecipations = pd.read_csv(qualifyingUrl, sep=',', index_col='qualifyId')\n",
    "# cast year to int. If type(year) = str --> Literal= year-01-01\n",
    "# movies.astype({'year': 'int32'}).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b3a7a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"fo\", FO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "465d6aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 6.48 s\n",
      "Wall time: 9.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the movies dataframe\n",
    "for index, row in q_partecipations.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the movie id as URI\n",
    "    Q_partecipation = URIRef(FO[\"q_partecipation\"+str(index)])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Q_partecipation, RDF.type, FO.QualifPartecipation))\n",
    "    g.add((Q_partecipation, FO['hasCarNumber'], Literal(row['number'], datatype=XSD.integer)))\n",
    "    g.add((Q_partecipation, FO['hasPosition'], Literal(row['position'], datatype=XSD.integer)))\n",
    "    if(str(row['q1']) != '\\\\N'):\n",
    "        g.add((Q_partecipation, FO['hasQ1Time'], Literal(\"00:0\"+str(row['q1']), datatype=XSD.time)))\n",
    "    if(str(row['q2']) != '\\\\N'):\n",
    "        g.add((Q_partecipation, FO['hasQ2Time'], Literal(\"00:0\"+str(row['q2']), datatype=XSD.time)))\n",
    "    if(str(row['q3']) != '\\\\N'):\n",
    "        g.add((Q_partecipation, FO['hasQ3Time'], Literal(\"00:0\"+str(row['q3']), datatype=XSD.time)))\n",
    "\n",
    "    Driver = URIRef(FO[\"driver\"+str(row['driverId'])])\n",
    "    g.add((Q_partecipation, FO['hasDriver'], Driver))\n",
    "    \n",
    "    Constructor = URIRef(FO[\"constructor\"+str(row['constructorId'])])\n",
    "    g.add((Q_partecipation, FO['hasConstructor'], Constructor))\n",
    "    \n",
    "    Qualifying = URIRef(FO[\"qualifying\"+str(row['raceId'])])\n",
    "    g.add((Qualifying, RDF.type, FO.Qualifying))\n",
    "    g.add((Q_partecipation, FO['partecipatedInQualif'], Qualifying))\n",
    "    \n",
    "    Q_date_time = races[races.index == row['raceId']]\n",
    "    if(str(Q_date_time['quali_date'].values[0]) != \"\\\\N\"):\n",
    "        g.add((Qualifying, FO['hasQualiDate'], Literal(Q_date_time['quali_date'].values[0], datatype=XSD.date)))\n",
    "    if(str(Q_date_time['quali_time'].values[0]) != \"\\\\N\"):\n",
    "        g.add((Qualifying, FO['hasQualiTime'], Literal(Q_date_time['quali_time'].values[0], datatype=XSD.time)))\n",
    "    \n",
    "    Race = URIRef(FO[\"race\"+str(row['raceId'])])\n",
    "    g.add((Race, FO['hasA'], Qualifying))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "56abe1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 2.31 s\n",
      "Wall time: 3.99 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'qualifying.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4ff574",
   "metadata": {},
   "source": [
    "# Sprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9271534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "s_partecipations = pd.read_csv(sprint_resultsUrl, sep=',', index_col='resultId')\n",
    "# cast year to int. If type(year) = str --> Literal= year-01-01\n",
    "# movies.astype({'year': 'int32'}).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "af17e29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"fo\", FO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f6544469",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 156 ms\n",
      "Wall time: 196 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the movies dataframe\n",
    "for index, row in s_partecipations.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + the movie id as URI\n",
    "    S_partecipation = URIRef(FO[\"s_partecipation\"+str(index)])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((S_partecipation, RDF.type, FO.SprintPartecipation))\n",
    "    g.add((S_partecipation, FO['hasCarNumber'], Literal(row['number'], datatype=XSD.integer)))\n",
    "    g.add((S_partecipation, FO['hasStartingGridPosition'], Literal(row['grid'], datatype=XSD.integer)))\n",
    "    if(str(row['position']) != '\\\\N'):\n",
    "        g.add((S_partecipation, FO['hasPosition'], Literal(row['position'], datatype=XSD.integer)))\n",
    "    g.add((S_partecipation, FO['hasPositionText'], Literal(row['positionText'], datatype=XSD.string)))\n",
    "    g.add((S_partecipation, FO['hasPositionOrder'], Literal(row['positionOrder'], datatype=XSD.integer)))\n",
    "    g.add((S_partecipation, FO['hasPoints'], Literal(row['points'], datatype=XSD.integer)))\n",
    "    g.add((S_partecipation, FO['hasLaps'], Literal(row['laps'], datatype=XSD.integer)))\n",
    "    if(str(row['time']) != '\\\\N'):\n",
    "        g.add((S_partecipation, FO['hasResultTime'], Literal(row['time'], datatype=XSD.string)))\n",
    "    if(str(row['milliseconds']) != '\\\\N'):\n",
    "        g.add((S_partecipation, FO['hasMillisecondsResultTime'], Literal(row['milliseconds'], datatype=XSD.integer)))\n",
    "    if(str(row['fastestLap']) != '\\\\N'):\n",
    "        g.add((S_partecipation, FO['hasFastestLap'], Literal(row['fastestLap'], datatype=XSD.integer)))\n",
    "    if(str(row['fastestLapTime']) != '\\\\N'):\n",
    "        g.add((S_partecipation, FO['hasFastestLapTime'], Literal(\"00:0\"+str(row['fastestLapTime']), datatype=XSD.time)))\n",
    "    \n",
    "    Driver = URIRef(FO[\"driver\"+str(row['driverId'])])\n",
    "    g.add((S_partecipation, FO['hasDriver'], Driver))\n",
    "    \n",
    "    Constructor = URIRef(FO[\"constructor\"+str(row['constructorId'])])\n",
    "    g.add((S_partecipation, FO['hasConstructor'], Constructor))\n",
    "    \n",
    "    Sprint = URIRef(FO[\"sprint\"+str(row['raceId'])])\n",
    "    g.add((Sprint, RDF.type, FO.Sprint))\n",
    "    g.add((S_partecipation, FO['partecipatedInSprint'], Sprint))\n",
    "    \n",
    "    S_date_time = races[races.index == row['raceId']]\n",
    "    if(str(S_date_time['sprint_date'].values[0]) != \"\\\\N\"):\n",
    "        g.add((Sprint, FO['hasSprintDate'], Literal(S_date_time['sprint_date'].values[0], datatype=XSD.date)))\n",
    "    if(str(S_date_time['sprint_time'].values[0]) != \"\\\\N\"):\n",
    "        g.add((Sprint, FO['hasSprintTime'], Literal(S_date_time['sprint_time'].values[0], datatype=XSD.time)))\n",
    "    \n",
    "    Race = URIRef(FO[\"race\"+str(row['raceId'])])\n",
    "    g.add((Race, FO['hasA'], Sprint))\n",
    "    \n",
    "    Status = URIRef(FO[\"status\"+str(row['statusId'])])\n",
    "    g.add((S_partecipation, FO['hasStatus'], Status))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "111ccaf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 78.1 ms\n",
      "Wall time: 117 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'sprint.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
