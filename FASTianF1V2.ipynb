{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd6904cd",
   "metadata": {},
   "source": [
    "## Populate FASTianF1 RDF database\n",
    "\n",
    "This notebook reports the main steps to download CSV files, process them and create an RDF dataset from them accordingly to an ontology.\n",
    "\n",
    "To measure execution time in Jupyter notebooks: <code>pip install ipython-autotime</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b01ea34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# required libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c483ac7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the required libraries\n",
    "from rdflib import Graph, Literal, RDF, URIRef, Namespace\n",
    "# rdflib knows about some namespaces, like FOAF\n",
    "from rdflib.namespace import FOAF, XSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c483db2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK DATE \n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5fa963",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5543c38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\Documents\\INGEGNERIA\\ANNO 5\\DB 2\\FASTianF1\n"
     ]
    }
   ],
   "source": [
    "# parameters and URLs\n",
    "print(str(Path(os.path.abspath(os.getcwd())).parent.absolute()))\n",
    "path = str(Path(os.path.abspath(os.getcwd())).parent.absolute())\n",
    "\n",
    "circuitsUrl = path + '\\FASTianF1\\data\\DatasetF1\\circuits.csv'\n",
    "constructor_resultsUrl = path + '\\FASTianF1\\data\\DatasetF1\\constructor_results.csv'\n",
    "constructor_standingsUrl = path + '\\FASTianF1\\data\\DatasetF1\\constructor_standings.csv'\n",
    "constructorsUrl = path + '\\FASTianF1\\data\\DatasetF1\\constructors.csv'\n",
    "driver_standingsUrl = path + '\\FASTianF1\\data\\DatasetF1\\driver_standings.csv'\n",
    "driversUrl = path + '\\FASTianF1\\data\\DatasetF1\\drivers.csv'\n",
    "lap_timesUrl = path + '\\FASTianF1\\data\\DatasetF1\\lap_times.csv'\n",
    "pit_stopsUrl = path + '\\FASTianF1\\data\\DatasetF1\\pit_stops.csv'\n",
    "qualifyingUrl = path + '\\FASTianF1\\data\\DatasetF1\\qualifying.csv'\n",
    "racesUrl = path + '\\FASTianF1\\data\\DatasetF1\\\\races.csv'\n",
    "resultsUrl = path + '\\FASTianF1\\data\\DatasetF1\\\\results.csv'\n",
    "sprint_resultsUrl = path + '\\FASTianF1\\data\\DatasetF1\\sprint_results.csv'\n",
    "statusUrl = path + '\\FASTianF1\\data\\DatasetF1\\status.csv'\n",
    "ratingsUrl = path + '\\FASTianF1\\data\\DatasetF1\\\\ratings.csv'\n",
    "seasonsUrl = path + '\\FASTianF1\\data\\DatasetF1\\seasons.csv'\n",
    "\n",
    "# country codes and nationalities conversion\n",
    "countriesURL = path + '\\FASTianF1\\data\\countryCodes\\wikipedia-iso-country-codes.csv'\n",
    "nationalitiesURL = path + '\\FASTianF1\\data\\countryCodes\\\\nationalities.csv'\n",
    "\n",
    "# saving folder\n",
    "savePath =  path + '\\FASTianF1\\data\\\\rdf\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806e71b5",
   "metadata": {},
   "source": [
    "# Namespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf08a3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the country and the movie ontology namespaces not known by RDFlib\n",
    "CNS = Namespace(\"http://eulersharp.sourceforge.net/2003/03swap/countries#\")\n",
    "FO = Namespace(\"https://www.dei.unipd.it/db2/groupProject/FASTianF1#\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7880d54",
   "metadata": {},
   "source": [
    "# Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdf231c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\chris\\\\Documents\\\\INGEGNERIA\\\\ANNO 5\\\\DB 2\\\\FASTianF1\\\\FASTianF1\\\\data\\\\countryCodes\\\\wikipedia-iso-country-codes.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#load the country codes\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# we need to convert NaN values to something else otherwise NA strings are converted to NaN -> problem with Namibia\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m countries \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcountriesURL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEnglish short name lower case\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1213\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    788\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\chris\\\\Documents\\\\INGEGNERIA\\\\ANNO 5\\\\DB 2\\\\FASTianF1\\\\FASTianF1\\\\data\\\\countryCodes\\\\wikipedia-iso-country-codes.csv'"
     ]
    }
   ],
   "source": [
    "#load the country codes\n",
    "# we need to convert NaN values to something else otherwise NA strings are converted to NaN -> problem with Namibia\n",
    "countries = pd.read_csv(countriesURL, sep=',', index_col='English short name lower case', keep_default_na=False, na_values=['_'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b8b325",
   "metadata": {},
   "source": [
    "# Drivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93a236e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "drivers = pd.read_csv(driversUrl, sep=',', index_col='driverId')\n",
    "nationalities = pd.read_csv(nationalitiesURL, sep=',', index_col='num_code')\n",
    "# cast year to int. If type(year) = str --> Literal= year-01-01\n",
    "# movies.astype({'year': 'int32'}).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "965c1b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"fo\", FO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "077e008d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 438 ms\n",
      "Wall time: 1 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the drivers dataframe\n",
    "for index, row in drivers.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + \"driver\" + the driver id as URI\n",
    "    Driver = URIRef(FO[\"driver\"+str(index)])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Driver, RDF.type, FO.Driver))\n",
    "    g.add((Driver, FO['hasDriverRef'], Literal(row['driverRef'], datatype=XSD.string)))\n",
    "    if(str(row['number']) != '\\\\N'):\n",
    "        g.add((Driver, FO['hasDriverNumber'], Literal(row['number'], datatype=XSD.integer)))\n",
    "    if(str(row['code']) != '\\\\N'):\n",
    "        g.add((Driver, FO['hasCode'], Literal(row['code'], datatype=XSD.string)))\n",
    "    g.add((Driver, FO['hasForename'], Literal(row['forename'], datatype=XSD.string)))\n",
    "    g.add((Driver, FO['hasSurname'], Literal(row['surname'], datatype=XSD.string)))\n",
    "    g.add((Driver, FO['hasURL'], Literal(row['url'], datatype=XSD.string)))\n",
    "    \n",
    "    #Check that the date has the year-month-day format otherwise print error\n",
    "    try:\n",
    "        datetime.datetime.strptime(str(row['dob']), '%Y-%m-%d')\n",
    "        g.add((Driver, FO['hasDateOfBirth'], Literal(row['dob'], datatype=XSD.date)))\n",
    "    except ValueError:\n",
    "        print(\"Incorrect date format\")\n",
    "\n",
    "    ## handle nationality\n",
    "    # there can be more than one nationality per driver\n",
    "    for nationality in str(row['nationality']).split('-'):\n",
    "        nationalityName = nationality.strip()\n",
    "        # check if the nationality exists in the nationalities dataframe\n",
    "        # str.contains() returns an array of booleans, thus we need to use the any() method\n",
    "        if((nationalities['nationality'].str.contains(nationalityName, case=False)).any() == True):\n",
    "            #get the country code, convert to string and get the lower case to match the country codes in the ontology\n",
    "            #There are multiple countries that correspond to American nationality, then the code for Americans is manually set to \"us\"\n",
    "            if(nationalityName != \"American\"):\n",
    "                code = str(nationalities[nationalities['nationality'].str.contains(nationalityName, case=False)]['alpha_2_code'].values[0]).lower()\n",
    "            else:\n",
    "                code = \"us\"\n",
    "            # create the RDF node for Country\n",
    "            Country = URIRef(CNS[code])\n",
    "            # add the edge connecting the Driver and the Country \n",
    "            g.add((Driver, FO['hasNation'], Country))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ff9af26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 46.9 ms\n",
      "Wall time: 271 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'drivers.ttl', 'w', encoding='utf-8') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa815a7",
   "metadata": {},
   "source": [
    "# Circuits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6e32881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "circuits = pd.read_csv(circuitsUrl, sep=',', index_col='circuitId')\n",
    "# cast year to int. If type(year) = str --> Literal= year-01-01\n",
    "# movies.astype({'year': 'int32'}).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "026c7a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"fo\", FO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d99e3dd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 74.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "substitutions = {'ó': 'o', 'ü': 'u', 'ã': 'a', ' ': ''}\n",
    "substitutions2 = {'UAE': 'United Arab Emirates', 'USA': 'United States', 'UK': 'United Kingdom', 'Russia': 'Russian Federation', 'Korea': 'Korea, Republic of'}\n",
    "\n",
    "#iterate over the circuits dataframe\n",
    "for index, row in circuits.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + \"circuits\" + the circuit id as URI\n",
    "    Circuit = URIRef(FO[\"circuit\"+str(index)])\n",
    "    loc = str(row['location'])\n",
    "    # substitution of special characters with standard characters\n",
    "    # special characters are not allowed in URIs\n",
    "    for old, new in substitutions.items():\n",
    "        loc = loc.replace(old, new)\n",
    "    # create the RDF node for location\n",
    "    Location = URIRef(FO[\"location\"+loc])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Location, RDF.type, FO.Location))\n",
    "    g.add((Circuit, RDF.type, FO.Circuit))\n",
    "    g.add((Circuit, FO['hasCircuitRef'], Literal(row['circuitRef'], datatype=XSD.string)))\n",
    "    g.add((Circuit, FO['hasName'], Literal(row['name'], datatype=XSD.string)))\n",
    "    g.add((Circuit, FO['hasLat'], Literal(row['lat'], datatype=XSD.float)))\n",
    "    g.add((Circuit, FO['hasLng'], Literal(row['lng'], datatype=XSD.float)))\n",
    "    if(str(row['alt']) != '\\\\N'):\n",
    "        g.add((Circuit, FO['hasAlt'], Literal(row['alt'], datatype=XSD.float)))\n",
    "    g.add((Circuit, FO['hasURL'], Literal(row['url'], datatype=XSD.string)))\n",
    "    # add the edge connecting the Circuit and the Location \n",
    "    g.add((Circuit, FO['hasLocation'], Location))\n",
    "\n",
    "    ## handle country\n",
    "    countryName = str(row['country'])\n",
    "    # substitution of abbreviations in country names for full names\n",
    "    for old, new in substitutions2.items():\n",
    "        countryName = countryName.replace(old, new)\n",
    "    # check if the country exists\n",
    "    # str.contains() returns an array of booleans, thus we need to use the any() method\n",
    "    if((countries.index == countryName).any() == True):\n",
    "        #get the country code, convert to string and get the lower case to match the country codes in the ontology \n",
    "        code = str(countries[countries.index == countryName]['Alpha-2 code'][0]).lower()\n",
    "        # create the RDF node for Country\n",
    "        Country = URIRef(CNS[code])\n",
    "        # add the edge connecting the Location and the Country \n",
    "        g.add((Location, FO['hasCountry'], Country))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1e8fedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 52.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'circuits.ttl', 'w', encoding='utf-8') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab133277",
   "metadata": {},
   "source": [
    "# Constructors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6908695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "constructors = pd.read_csv(constructorsUrl, sep=',', index_col='constructorId')\n",
    "# cast year to int. If type(year) = str --> Literal= year-01-01\n",
    "# movies.astype({'year': 'int32'}).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63c69926",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"fo\", FO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8612dfde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 62.5 ms\n",
      "Wall time: 221 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the constructors dataframe\n",
    "for index, row in constructors.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + \"constructor\" + the constructor id as URI\n",
    "    Constructor = URIRef(FO[\"constructor\"+str(index)])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Constructor, RDF.type, FO.Constructor))\n",
    "    g.add((Constructor, FO['hasConstructorRef'], Literal(row['constructorRef'], datatype=XSD.string)))\n",
    "    g.add((Constructor, FO['hasName'], Literal(row['name'], datatype=XSD.string)))\n",
    "    g.add((Constructor, FO['hasURL'], Literal(row['url'], datatype=XSD.string)))\n",
    "\n",
    "    ## handle nationality\n",
    "    #there can be more than one nationality per constructor\n",
    "    for nationality in str(row['nationality']).split('-'):\n",
    "        nationalityName = nationality.strip()\n",
    "        # check if the nationality exists\n",
    "        # str.contains() returns an array of booleans, thus we need to use the any() method\n",
    "        if((nationalities['nationality'].str.contains(nationalityName, case=False)).any() == True):\n",
    "            #get the country code, convert to string and get the lower case to match the country codes in the ontology \n",
    "            #There are multiple countries that correspond to American nationality, then the code for Americans is manually set to \"us\"\n",
    "            if(nationalityName != \"American\"):\n",
    "                code = str(nationalities[nationalities['nationality'].str.contains(nationalityName, case=False)]['alpha_2_code'].values[0]).lower()\n",
    "            else:\n",
    "                code = \"us\"\n",
    "            # create the RDF node for country\n",
    "            Country = URIRef(CNS[code])\n",
    "            # add the edge connecting the Constructor and the Country \n",
    "            g.add((Constructor, FO['hasNation'], Country))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59913aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 72.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'constructors.ttl', 'w', encoding='utf-8') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577738ca",
   "metadata": {},
   "source": [
    "# Status and Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29c60ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "status = pd.read_csv(statusUrl, sep=',', index_col='statusId')\n",
    "seasons = pd.read_csv(seasonsUrl, sep=',', index_col='year')\n",
    "# cast year to int. If type(year) = str --> Literal= year-01-01\n",
    "# movies.astype({'year': 'int32'}).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c386e7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 38.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the status dataframe\n",
    "for index, row in status.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + \"status\" + the status id as URI\n",
    "    Status = URIRef(FO[\"status\"+str(index)])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Status, RDF.type, FO.Status))\n",
    "    g.add((Status, FO['hasName'], Literal(row['status'], datatype=XSD.string)))\n",
    "    \n",
    "#iterate over the seasons dataframe\n",
    "for index, row in seasons.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + \"season\" + the season id as URI\n",
    "    Season = URIRef(FO[\"season\"+str(index)])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Season, RDF.type, FO.Season))\n",
    "    g.add((Season, FO['hasYear'], Literal(int(index), datatype=XSD.integer)))\n",
    "    g.add((Season, FO['hasURL'], Literal(row['url'], datatype=XSD.string)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34df99ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 75 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'status_seasons.ttl', 'w', encoding='utf-8') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dcd70b",
   "metadata": {},
   "source": [
    "# Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67e22c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "races = pd.read_csv(racesUrl, sep=',', index_col='raceId')\n",
    "# cast year to int. If type(year) = str --> Literal= year-01-01\n",
    "# movies.astype({'year': 'int32'}).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "687a35ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"fo\", FO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "340c39af",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 156 ms\n",
      "Wall time: 382 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the races dataframe\n",
    "for index, row in races.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + \"race\" + the race id as URI\n",
    "    Race = URIRef(FO[\"race\"+str(index)])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Race, RDF.type, FO.Race))\n",
    "    g.add((Race, FO['hasRound'], Literal(row['round'], datatype=XSD.integer)))\n",
    "    g.add((Race, FO['hasName'], Literal(row['name'], datatype=XSD.string)))\n",
    "    g.add((Race, FO['hasDate'], Literal(row['date'], datatype=XSD.date)))\n",
    "    if(str(row['time']) != '\\\\N'):\n",
    "        g.add((Race, FO['hasTime'], Literal(row['time'], datatype=XSD.time)))\n",
    "    g.add((Race, FO['hasURL'], Literal(row['url'], datatype=XSD.string)))\n",
    "    \n",
    "    if(str(row['fp1_date']) != '\\\\N'):\n",
    "        g.add((Race, FO['hasFp1Date'], Literal(row['fp1_date'], datatype=XSD.date)))\n",
    "    if(str(row['fp1_time']) != '\\\\N'):\n",
    "        g.add((Race, FO['hasFp1Time'], Literal(row['fp1_time'], datatype=XSD.time)))\n",
    "    if(str(row['fp2_date']) != '\\\\N'):\n",
    "        g.add((Race, FO['hasFp2Date'], Literal(row['fp2_date'], datatype=XSD.date)))\n",
    "    if(str(row['fp2_time']) != '\\\\N'):\n",
    "        g.add((Race, FO['hasFp2Time'], Literal(row['fp2_time'], datatype=XSD.time)))\n",
    "    if(str(row['fp3_date']) != '\\\\N'):\n",
    "        g.add((Race, FO['hasFp3Date'], Literal(row['fp3_date'], datatype=XSD.date)))\n",
    "    if(str(row['fp3_time']) != '\\\\N'):\n",
    "        g.add((Race, FO['hasFp3Time'], Literal(row['fp3_time'], datatype=XSD.time)))\n",
    "    \n",
    "    # create the RDF node for circuit\n",
    "    Circuit = URIRef(FO[\"circuit\"+str(row['circuitId'])])\n",
    "    # add the edge connecting the Race and the Circuit \n",
    "    g.add((Race, FO['hasCircuit'], Circuit))\n",
    "    \n",
    "    # create the RDF node for season\n",
    "    Season = URIRef(FO[\"season\"+str(row['year'])])\n",
    "    # add the edge connecting the Race and the Season \n",
    "    g.add((Race, FO['inSeason'], Season))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "07d2285d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 188 ms\n",
      "Wall time: 372 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'race.ttl', 'w', encoding='utf-8') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f27d02",
   "metadata": {},
   "source": [
    "# Race (partecipations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e5804573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "r_partecipations = pd.read_csv(resultsUrl, sep=',', index_col='resultId')\n",
    "constructor_results = pd.read_csv(constructor_resultsUrl, sep=',', index_col=\"constructorResultsId\")\n",
    "# join is a dataframe containing the outer join of participation results (r_partecipations) \n",
    "# and driver results (driver_standings)\n",
    "# raceId and driverId are used as join keys\n",
    "join = r_partecipations.merge(constructor_results, how='left', on=['raceId','constructorId'], suffixes=('', 'Constructor')).fillna('\\\\N')\n",
    "laps = pd.read_csv(lap_timesUrl, sep=',')\n",
    "# cast year to int. If type(year) = str --> Literal= year-01-01\n",
    "# movies.astype({'year': 'int32'}).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6cf33c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"fo\", FO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2d47e1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that transforms a time to standard %H:%M:%S.%f format, \n",
    "# adds the zeros and the missing colon to the beginning and end of the string.\n",
    "def time_formatter(splitted_time):\n",
    "    return \"00:00:00\"[0:8-len(splitted_time[0])] + splitted_time[0] + \".\" + splitted_time[1].ljust(3,\"0\")\n",
    "\n",
    "# Function that transforms a gap time to standard %H:%M:%S.%f format, \n",
    "# adds the zeros and the missing colon to the beginning and end of the string.\n",
    "def gap_formatter(gap: str):\n",
    "    splitted_gap = gap.strip().split('.')\n",
    "    if ((':' not in splitted_gap[0]) and (int(splitted_gap[0])>59)):\n",
    "        delta = datetime.timedelta(seconds = int(splitted_gap[0]))\n",
    "        h, mod = divmod(delta.seconds, 3600)\n",
    "        m, s = divmod(mod, 60)\n",
    "        #print(\"{:02d}:{:02d}:{:02d}.{:03d}\".format(h, m, s, int(splitted_gap[1].ljust(3,\"0\"))))\n",
    "        return \"{:02d}:{:02d}:{:02d}.{:03d}\".format(h, m, s, int(splitted_gap[1].ljust(3,\"0\")))\n",
    "    else:\n",
    "        return time_formatter(splitted_gap)\n",
    "\n",
    "# Function that calculates a driver's actual arrival time using the winner's arrival time and the time distance from it.\n",
    "def time_converter(time_gap: str,race: str,pos: str):\n",
    "    \n",
    "    #FIRST_TIME RETRIEVAL\n",
    "    #print(\"FIRST_TIME\", end=\" \")\n",
    "    tmp = join[(join['raceId'] == int(race)) & (join['positionText'] == '1')]\n",
    "    #print(\"Error\") if (tmp.shape[0]!=1) else None\n",
    "    first_splitted = str(tmp.iloc[0]['time']).strip().split('.')\n",
    "    first_time = datetime.datetime.strptime(time_formatter(first_splitted), \"%H:%M:%S.%f\")\n",
    "    \n",
    "    #DELTA RETRIEVAL\n",
    "    #print(\"DELTA\", end=\" \")\n",
    "    splitted_time_gap = time_gap.strip().split('.')\n",
    "    if ':' in (splitted_time_gap[0]):\n",
    "        formatted_time_gap = time_formatter(splitted_time_gap)\n",
    "        (h, m, s) = formatted_time_gap.split('.')[0].split(':')\n",
    "    else:\n",
    "        (h, m, s) = (0,0, splitted_time_gap[0])\n",
    "    delta = datetime.timedelta(hours=int(h), minutes=int(m), seconds=int(s), milliseconds=int(splitted_time_gap[1]))\n",
    "    \n",
    "    '''\n",
    "    print(\"NEW_TIME\")\n",
    "    if(int(race) < 22):\n",
    "        print(\"POSITION-->\",pos,\"FIRST TIME-->\",first_time, end=\" \")\n",
    "        print(\"DELTA-->\",delta,\"NEW TIME-->\",((first_time + delta).strftime(\"%H:%M:%S.%f\"))[:12])\n",
    "    '''\n",
    "    \n",
    "    return first_time + delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "85ae2206",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 32.7 s\n",
      "Wall time: 44.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the join dataframe\n",
    "for index, row in join.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + \"r_partecipation\" + the partecipation id as URI\n",
    "    R_partecipation = URIRef(FO[\"r_partecipation\"+str(index)])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((R_partecipation, RDF.type, FO.RacePartecipation))\n",
    "    if(str(row['number']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasCarNumber'], Literal(row['number'], datatype=XSD.integer)))\n",
    "    if(str(row['grid']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasStartingGridPosition'], Literal(int(row['grid']), datatype=XSD.integer)))\n",
    "    if(str(row['position']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasPosition'], Literal(row['position'], datatype=XSD.integer)))\n",
    "    if(str(row['positionText']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasPositionText'], Literal(row['positionText'], datatype=XSD.string)))\n",
    "    if(str(row['positionOrder']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasPositionOrder'], Literal(int(row['positionOrder']), datatype=XSD.integer)))\n",
    "    if(str(row['points']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasPoints'], Literal(int(row['points']), datatype=XSD.integer)))\n",
    "    if(str(row['laps']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasLaps'], Literal(int(row['laps']), datatype=XSD.integer)))\n",
    "    if((str(row['time']) != '\\\\N') and (str(row['time']) != '+1:10')):\n",
    "        #print(row['raceId'],row['driverId'], end=\"   \")\n",
    "        if row['time'][0] == \"+\":\n",
    "            new_time = time_converter(str(row['time'])[1:], str(row['raceId']),str(row['positionText']))\n",
    "            g.add((R_partecipation, FO['hasResultTime'], Literal(new_time.strftime(\"%H:%M:%S.%f\")[:12], datatype=XSD.time)))\n",
    "            g.add((R_partecipation, FO['hasResultGap'], Literal(gap_formatter(str(row['time'])[1:]), datatype=XSD.time)))\n",
    "        else:\n",
    "            splitted = str(row['time']).strip().split('.')\n",
    "            g.add((R_partecipation, FO['hasResultTime'], Literal(time_formatter(splitted), datatype=XSD.time)))\n",
    "            g.add((R_partecipation, FO['hasResultGap'], Literal(\"00:00:00.000\", datatype=XSD.time)))\n",
    "            #if(int(row['raceId']) < 22):\n",
    "                #print(\"POSITION-->\",str(row['positionText']),\"FIRST TIME-->\",str(row['time']))\n",
    "    if(str(row['milliseconds']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasMillisecondsResultTime'], Literal(row['milliseconds'], datatype=XSD.integer)))\n",
    "    if(str(row['fastestLap']) != '\\\\N'):\n",
    "        # Get the rows of the laps dataframe with the raceId, driverId and lap values matching those in the current row.\n",
    "        tmp = laps[(laps['raceId'] == row['raceId']) & (laps['driverId'] == row['driverId']) & (laps['lap'] == int(row['fastestLap']))]\n",
    "        #iterate over the rows found\n",
    "        for index2, row2 in tmp.iterrows():\n",
    "            # create the RDF node for lap\n",
    "            Lap = URIRef(FO[\"lap\"+str(index2)])\n",
    "            # add the edge connecting the R_partecipation and the Lap \n",
    "            g.add((R_partecipation, FO['hasFastestLap'], Lap))\n",
    "    if(str(row['rank']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasFastestLapRank'], Literal(row['rank'], datatype=XSD.integer)))\n",
    "    if(str(row['fastestLapTime']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasFastestLapTime'], Literal(\"00:00:00.000\"[0:12-len(str(row['fastestLapTime']))] + \n",
    "                                                                 str(row['fastestLapTime']), datatype=XSD.time)))\n",
    "    if(str(row['fastestLapSpeed']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasFastestLapSpeed'], Literal(float(row['fastestLapSpeed']), datatype=XSD.decimal)))\n",
    "    if(str(row['pointsConstructor']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasConstructorPoints'], Literal(int(row['pointsConstructor']), datatype=XSD.integer)))\n",
    "    \n",
    "    # create the RDF node for driver\n",
    "    Driver = URIRef(FO[\"driver\"+str(row['driverId'])])\n",
    "    # add the edge connecting the Partecipation and the Driver \n",
    "    g.add((R_partecipation, FO['hasDriver'], Driver))\n",
    "    \n",
    "    if(str(row['constructorId']) != '\\\\N'):\n",
    "        # create the RDF node for constructor\n",
    "        Constructor = URIRef(FO[\"constructor\"+str(int(row['constructorId']))])\n",
    "        # add the edge connecting the Partecipation and the Constructor \n",
    "        g.add((R_partecipation, FO['hasConstructor'], Constructor))\n",
    "    \n",
    "    # create the RDF node for race\n",
    "    Race = URIRef(FO[\"race\"+str(row['raceId'])])\n",
    "    # add the edge connecting the Partecipation and the Race \n",
    "    g.add((R_partecipation, FO['partecipatedInRace'], Race))\n",
    "    \n",
    "    if(str(row['statusId']) != '\\\\N'):\n",
    "        # create the RDF node for status\n",
    "        Status = URIRef(FO[\"status\"+str(int(row['statusId']))])\n",
    "        # add the edge connecting the Partecipation and the Status \n",
    "        g.add((R_partecipation, FO['hasStatus'], Status))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "75c2c025",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 8.86 s\n",
      "Wall time: 11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'race_partecipations.ttl', 'w', encoding='utf-8') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2233e44",
   "metadata": {},
   "source": [
    "# Qualifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd044a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "q_partecipations = pd.read_csv(qualifyingUrl, sep=',', index_col='qualifyId')\n",
    "# cast year to int. If type(year) = str --> Literal= year-01-01\n",
    "# movies.astype({'year': 'int32'}).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b3a7a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"fo\", FO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "465d6aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 4.94 s\n",
      "Wall time: 5.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the q_partecipations dataframe\n",
    "for index, row in q_partecipations.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + \"q_partecipation\" + the qualifying partecipation id as URI\n",
    "    Q_partecipation = URIRef(FO[\"q_partecipation\"+str(index)])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Q_partecipation, RDF.type, FO.QualifPartecipation))\n",
    "    g.add((Q_partecipation, FO['hasCarNumber'], Literal(row['number'], datatype=XSD.integer)))\n",
    "    g.add((Q_partecipation, FO['hasPosition'], Literal(row['position'], datatype=XSD.integer)))\n",
    "    if(str(row['q1']) != '\\\\N'):\n",
    "        g.add((Q_partecipation, FO['hasQ1Time'], Literal(\"00:00:00.000\"[0:12-len(str(row['q1']))] + \n",
    "                                                         str(row['q1']), datatype=XSD.time)))\n",
    "    if(str(row['q2']) != '\\\\N'):\n",
    "        g.add((Q_partecipation, FO['hasQ2Time'], Literal(\"00:00:00.000\"[0:12-len(str(row['q2']))] + \n",
    "                                                         str(row['q2']), datatype=XSD.time)))\n",
    "    if(str(row['q3']) != '\\\\N'):\n",
    "        g.add((Q_partecipation, FO['hasQ3Time'], Literal(\"00:00:00.000\"[0:12-len(str(row['q3']))] + \n",
    "                                                         str(row['q3']), datatype=XSD.time)))\n",
    "\n",
    "    # create the RDF node for driver\n",
    "    Driver = URIRef(FO[\"driver\"+str(row['driverId'])])\n",
    "    # add the edge connecting the qualifyingPartecipation and the Driver \n",
    "    g.add((Q_partecipation, FO['hasDriver'], Driver))\n",
    "    \n",
    "    # create the RDF node for constructor\n",
    "    Constructor = URIRef(FO[\"constructor\"+str(row['constructorId'])])\n",
    "    # add the edge connecting the Partecipation and the Constructor \n",
    "    g.add((Q_partecipation, FO['hasConstructor'], Constructor))\n",
    "    \n",
    "    # create the RDF node for qualifying\n",
    "    Qualifying = URIRef(FO[\"qualifying\"+str(row['raceId'])])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Qualifying, RDF.type, FO.Qualifying))\n",
    "    # add the edge connecting the qualifyingPartecipation and the Qualifying \n",
    "    g.add((Q_partecipation, FO['partecipatedInQualif'], Qualifying))\n",
    "    \n",
    "    # create the RDF node for race\n",
    "    Race = URIRef(FO[\"race\"+str(row['raceId'])])\n",
    "    # add the edge connecting the Race and the Qualifying \n",
    "    g.add((Race, FO['hasA'], Qualifying))\n",
    "    \n",
    "    # Qualifying starting dates and times are stored in the races dataframe,\n",
    "    # then we retrieve them using the matching raceId\n",
    "    Q_date_time = races[races.index == row['raceId']]\n",
    "    if(str(Q_date_time['quali_date'].values[0]) != \"\\\\N\"):\n",
    "        g.add((Qualifying, FO['hasDate'], Literal(Q_date_time['quali_date'].values[0], datatype=XSD.date)))\n",
    "    if(str(Q_date_time['quali_time'].values[0]) != \"\\\\N\"):\n",
    "        g.add((Qualifying, FO['hasTime'], Literal(Q_date_time['quali_time'].values[0], datatype=XSD.time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "56abe1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 1.8 s\n",
      "Wall time: 2.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'qualifying.ttl', 'w', encoding='utf-8') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4ff574",
   "metadata": {},
   "source": [
    "# Sprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9271534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "s_partecipations = pd.read_csv(sprint_resultsUrl, sep=',', index_col='resultId')\n",
    "# cast year to int. If type(year) = str --> Literal= year-01-01\n",
    "# movies.astype({'year': 'int32'}).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "af17e29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"fo\", FO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f6544469",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 391 ms\n",
      "Wall time: 525 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the s_partecipations dataframe\n",
    "for index, row in s_partecipations.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + \"s_partecipation\" + the sprint partecipation id as URI\n",
    "    S_partecipation = URIRef(FO[\"s_partecipation\"+str(index)])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((S_partecipation, RDF.type, FO.SprintPartecipation))\n",
    "    g.add((S_partecipation, FO['hasCarNumber'], Literal(row['number'], datatype=XSD.integer)))\n",
    "    g.add((S_partecipation, FO['hasStartingGridPosition'], Literal(row['grid'], datatype=XSD.integer)))\n",
    "    if(str(row['position']) != '\\\\N'):\n",
    "        g.add((S_partecipation, FO['hasPosition'], Literal(row['position'], datatype=XSD.integer)))\n",
    "    g.add((S_partecipation, FO['hasPositionText'], Literal(row['positionText'], datatype=XSD.string)))\n",
    "    g.add((S_partecipation, FO['hasPositionOrder'], Literal(row['positionOrder'], datatype=XSD.integer)))\n",
    "    g.add((S_partecipation, FO['hasPoints'], Literal(row['points'], datatype=XSD.integer)))\n",
    "    g.add((S_partecipation, FO['hasLaps'], Literal(row['laps'], datatype=XSD.integer)))\n",
    "    if(str(row['time']) != '\\\\N'):\n",
    "        if row['time'][0] == \"+\":\n",
    "            new_time = time_converter(str(row['time'])[1:], str(row['raceId']),str(row['positionText']))\n",
    "            g.add((S_partecipation, FO['hasResultTime'], Literal(new_time.strftime(\"%H:%M:%S.%f\")[:12], datatype=XSD.time)))\n",
    "            g.add((S_partecipation, FO['hasResultGap'], Literal(gap_formatter(str(row['time'])[1:]), datatype=XSD.time)))\n",
    "        else:\n",
    "            splitted = str(row['time']).strip().split('.')\n",
    "            g.add((S_partecipation, FO['hasResultTime'], Literal(time_formatter(splitted), datatype=XSD.time)))\n",
    "            g.add((S_partecipation, FO['hasResultGap'], Literal(\"00:00:00.000\", datatype=XSD.time)))\n",
    "    if(str(row['milliseconds']) != '\\\\N'):\n",
    "        g.add((S_partecipation, FO['hasMillisecondsResultTime'], Literal(row['milliseconds'], datatype=XSD.integer)))\n",
    "    if(str(row['fastestLap']) != '\\\\N'):\n",
    "        g.add((S_partecipation, FO['hasFastestLap'], Literal(row['fastestLap'], datatype=XSD.integer)))\n",
    "    if(str(row['fastestLapTime']) != '\\\\N'):\n",
    "        g.add((S_partecipation, FO['hasFastestLapTime'], Literal(\"00:00:00.000\"[0:12-len(str(row['fastestLapTime']))] + \n",
    "                                                                 str(row['fastestLapTime']), datatype=XSD.time)))\n",
    "    \n",
    "    # create the RDF node for driver\n",
    "    Driver = URIRef(FO[\"driver\"+str(row['driverId'])])\n",
    "    # add the edge connecting the sprintPartecipation and the Driver\n",
    "    g.add((S_partecipation, FO['hasDriver'], Driver))\n",
    "    \n",
    "    # create the RDF node for constructor\n",
    "    Constructor = URIRef(FO[\"constructor\"+str(row['constructorId'])])\n",
    "    # add the edge connecting the sprintPartecipation and the Constructor\n",
    "    g.add((S_partecipation, FO['hasConstructor'], Constructor))\n",
    "    \n",
    "    # create the RDF node for sprint\n",
    "    Sprint = URIRef(FO[\"sprint\"+str(row['raceId'])])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Sprint, RDF.type, FO.Sprint))\n",
    "    # add the edge connecting the sprintPartecipation and the Sprint \n",
    "    g.add((S_partecipation, FO['partecipatedInSprint'], Sprint))\n",
    "    \n",
    "    # create the RDF node for race\n",
    "    Race = URIRef(FO[\"race\"+str(row['raceId'])])\n",
    "    # add the edge connecting the Race and the Sprint \n",
    "    g.add((Race, FO['hasA'], Sprint))\n",
    "    \n",
    "    # create the RDF node for status\n",
    "    Status = URIRef(FO[\"status\"+str(row['statusId'])])\n",
    "    # add the edge connecting the sprintPartecipation and the Sprint \n",
    "    g.add((S_partecipation, FO['hasStatus'], Status))\n",
    "    \n",
    "    # Sprint starting dates and times are stored in the races dataframe,\n",
    "    # then we retrieve them using the matching raceId\n",
    "    S_date_time = races[races.index == row['raceId']]\n",
    "    if(str(S_date_time['sprint_date'].values[0]) != \"\\\\N\"):\n",
    "        g.add((Sprint, FO['hasDate'], Literal(S_date_time['sprint_date'].values[0], datatype=XSD.date)))\n",
    "    if(str(S_date_time['sprint_time'].values[0]) != \"\\\\N\"):\n",
    "        g.add((Sprint, FO['hasTime'], Literal(S_date_time['sprint_time'].values[0], datatype=XSD.time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "111ccaf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 93.8 ms\n",
      "Wall time: 129 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'sprint.ttl', 'w', encoding='utf-8') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9183ef64",
   "metadata": {},
   "source": [
    "# Standings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "65f797ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "driver_standings = pd.read_csv(driver_standingsUrl, sep=',', index_col=\"driverStandingsId\")\n",
    "constructor_standings = pd.read_csv(constructor_standingsUrl, sep=',', index_col=\"constructorStandingsId\")\n",
    "# cast year to int. If type(year) = str --> Literal= year-01-01\n",
    "# movies.astype({'year': 'int32'}).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0bf337c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"fo\", FO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "304c9ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 8.67 s\n",
      "Wall time: 10.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the driver_standings dataframe\n",
    "for index, row in driver_standings.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + \"standing\" + the standing id as URI\n",
    "    Standing = URIRef(FO[\"d_standing\"+str(index)])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Standing, RDF.type, FO.DriverStanding))\n",
    "    g.add((Standing, FO['hasTotalPoints'], Literal(int(row['points']), datatype=XSD.integer)))\n",
    "    g.add((Standing, FO['hasTotalPosition'], Literal(int(row['position']), datatype=XSD.integer)))\n",
    "    g.add((Standing, FO['hasTotalPositionText'], Literal(row['positionText'], datatype=XSD.string)))\n",
    "    g.add((Standing, FO['hasDriversWins'], Literal(int(row['wins']), datatype=XSD.integer)))\n",
    "\n",
    "    # create the RDF node for driver\n",
    "    Driver = URIRef(FO[\"driver\"+str(row['driverId'])])\n",
    "    # add the edge connecting the Standing and the Driver \n",
    "    g.add((Standing, FO['hasDriver'], Driver))\n",
    "    \n",
    "    # create the RDF node for race\n",
    "    Race = URIRef(FO[\"race\"+str(row['raceId'])])\n",
    "    # add the edge connecting the Standing and the Race \n",
    "    g.add((Standing, FO['hasRace'], Race))\n",
    "    \n",
    "#iterate over the constructor_standings dataframe\n",
    "for index, row in constructor_standings.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + \"standing\" + the standing id as URI\n",
    "    Standing = URIRef(FO[\"c_standing\"+str(index)])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Standing, RDF.type, FO.ConstructorStanding))\n",
    "    g.add((Standing, FO['hasTotalPoints'], Literal(int(row['points']), datatype=XSD.integer)))\n",
    "    g.add((Standing, FO['hasTotalPosition'], Literal(int(row['position']), datatype=XSD.integer)))\n",
    "    g.add((Standing, FO['hasTotalPositionText'], Literal(row['positionText'], datatype=XSD.string)))\n",
    "    g.add((Standing, FO['hasWins'], Literal(int(row['wins']), datatype=XSD.integer)))\n",
    "\n",
    "    # create the RDF node for constructor\n",
    "    Constructor = URIRef(FO[\"constructor\"+str(row['constructorId'])])\n",
    "    # add the edge connecting the Standing and the Constructor\n",
    "    g.add((Standing, FO['hasConstructor'], Constructor))\n",
    "    \n",
    "    # create the RDF node for race\n",
    "    Race = URIRef(FO[\"race\"+str(row['raceId'])])\n",
    "    # add the edge connecting the Standing and the Race \n",
    "    g.add((Standing, FO['hasRace'], Race))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a4155476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 7.88 s\n",
      "Wall time: 13.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'standings.ttl', 'w', encoding='utf-8') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544125de",
   "metadata": {},
   "source": [
    "# Laps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c504383b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "# Csv already uploaded in Race partecipation section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "283ca1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"fo\", FO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bbb691b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 6min 16s\n",
      "Wall time: 7min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the laps dataframe\n",
    "for index, row in laps.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + \"lap\" + the lap id as URI\n",
    "    Lap = URIRef(FO[\"lap\"+str(index)])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Lap, RDF.type, FO.Lap))\n",
    "    g.add((Lap, FO['hasLapNumber'], Literal(row['lap'], datatype=XSD.integer)))\n",
    "    g.add((Lap, FO['hasLapPosition'], Literal(row['position'], datatype=XSD.integer)))\n",
    "    g.add((Lap, FO['hasLapTime'], Literal(\"00:00:00.000\"[0:12-len(str(row['time']))] + \n",
    "                                          str(row['time']), datatype=XSD.time)))\n",
    "    g.add((Lap, FO['hasMillisecondsTime'], Literal(row['milliseconds'], datatype=XSD.integer)))\n",
    "    \n",
    "    # Get the rows of the join dataframe with the raceId and driverId values matching those in the current row.\n",
    "    tmp = join[(join['raceId'] == row['raceId']) & (join['driverId'] == row['driverId'])]\n",
    "    #iterate over the rows found\n",
    "    for index2, row2 in tmp.iterrows():\n",
    "        # create the RDF node for racePartecipation\n",
    "        R_partecipation = URIRef(FO[\"r_partecipation\"+str(index2)])\n",
    "        # add the edge connecting the racePartecipation and the Lap \n",
    "        g.add((R_partecipation, FO['hasLap'], Lap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4be10ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 1min 31s\n",
      "Wall time: 1min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'laps.ttl', 'w', encoding='utf-8') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52bc8b3",
   "metadata": {},
   "source": [
    "# Pit stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "190fb428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "pit_stops = pd.read_csv(pit_stopsUrl, sep=',')\n",
    "# cast year to int. If type(year) = str --> Literal= year-01-01\n",
    "# movies.astype({'year': 'int32'}).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4c2ef890",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"fo\", FO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2d447acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 27.3 s\n",
      "Wall time: 35 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the pit_stops dataframe\n",
    "for index, row in pit_stops.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + \"stop\" + the movie id as URI\n",
    "    PitStop = URIRef(FO[\"stop\"+str(index)])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((PitStop, RDF.type, FO.PitStop))\n",
    "    g.add((PitStop, FO['hasStopNumber'], Literal(row['stop'], datatype=XSD.integer)))\n",
    "    g.add((PitStop, FO['hasPitStopTimeOfDay'], Literal(str(row['time']), datatype=XSD.time)))\n",
    "    g.add((PitStop, FO['hasMillisecondsTime'], Literal(row['milliseconds'], datatype=XSD.integer)))\n",
    "    g.add((PitStop, FO['hasDuration'], Literal(\"00:00:00.000\"[0:12-len(str(row['duration']))] + \n",
    "                                               str(row['duration']), datatype=XSD.time)))\n",
    "    \n",
    "    # Get the rows of the laps dataframe with the raceId, driverId and lap values matching those in the current row.\n",
    "    tmp = laps[(laps['raceId'] == row['raceId']) & (laps['driverId'] == row['driverId']) & (laps['lap'] == row['lap'])]\n",
    "    #iterate over the rows found\n",
    "    for index2, row2 in tmp.iterrows():\n",
    "        # create the RDF node for lap\n",
    "        Lap = URIRef(FO[\"lap\"+str(index2)])\n",
    "        # add the edge connecting the PitStop and the Lap \n",
    "        g.add((PitStop, FO['hasPitStopLap'], Lap))\n",
    "        \n",
    "    # Get the rows of the join dataframe with the raceId and driverId values matching those in the current row.\n",
    "    tmp2 = join[(join['raceId'] == row['raceId']) & (join['driverId'] == row['driverId'])]\n",
    "    #iterate over the rows found\n",
    "    for index2, row2 in tmp2.iterrows():\n",
    "        # create the RDF node for racePartecipation\n",
    "        R_partecipation = URIRef(FO[\"r_partecipation\"+str(index2)])\n",
    "        # add the edge connecting the racePartecipation and the PitStop \n",
    "        g.add((R_partecipation, FO['hasPitStop'], PitStop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eaa6d059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 2.08 s\n",
      "Wall time: 3.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'stops.ttl', 'w', encoding='utf-8') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fdf6c5",
   "metadata": {},
   "source": [
    "# Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8b46dbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "ratings = pd.read_csv(ratingsUrl, sep=',')\n",
    "# cast year to int. If type(year) = str --> Literal= year-01-01\n",
    "# movies.astype({'year': 'int32'}).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "feff869d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"fo\", FO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fad35fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 328 ms\n",
      "Wall time: 448 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the ratings dataframe\n",
    "for index, row in ratings.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + \"rating\" + the movie id as URI\n",
    "    Rating = URIRef(FO[\"rating\"+str(index)])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Rating, RDF.type, FO.Rating))\n",
    "    g.add((Rating, FO['hasPeriod'], Literal(row['Period'], datatype=XSD.date)))\n",
    "    g.add((Rating, FO['hasRating'], Literal(row['Rating'], datatype=XSD.integer)))\n",
    "    g.add((Rating, FO['hasExperience'], Literal(row['Experience'], datatype=XSD.integer)))\n",
    "    g.add((Rating, FO['hasRaceCraft'], Literal(row['Race Craft'], datatype=XSD.integer)))\n",
    "    g.add((Rating, FO['hasAwareness'], Literal(row['Awareness'], datatype=XSD.integer)))\n",
    "    g.add((Rating, FO['hasPace'], Literal(row['Pace'], datatype=XSD.integer)))\n",
    "    g.add((Rating, FO['hasContractCost'], Literal(row['Contract Cost'], datatype=XSD.long)))\n",
    "    g.add((Rating, FO['hasSalary'], Literal(row['Salary'], datatype=XSD.long)))\n",
    "    g.add((Rating, FO['hasBuyout'], Literal(row['Buyout'], datatype=XSD.long)))\n",
    "    \n",
    "    # create the RDF node for season\n",
    "    Season = URIRef(FO[\"season\"+str(row['Year'])])\n",
    "    # add the edge connecting the Rating and the Season \n",
    "    g.add((Rating, FO['inSeason'], Season))\n",
    "    \n",
    "    # in ratings csv names are full-names, then they are splitted in forename and surname\n",
    "    name = str(row['Driver']).split(' ')\n",
    "    forename = name[0].strip()\n",
    "    surname = name[1].strip()\n",
    "    # Get the rows of the drivers dataframe with the forename value (rating csv) contained in forename column of the current row (driver csv).\n",
    "    subCsv = drivers[drivers['forename'].str.contains(forename, case=False)]\n",
    "    # If exists at least one row in subCsv that contain also the surname of the rating\n",
    "    if((subCsv['surname'].str.contains(surname, case=False)).any() == True):\n",
    "        # Get the rows of the drivers dataframe with the forename and surname values contained in the matching column of the current row.\n",
    "        dId = drivers[(drivers['forename'].str.contains(forename, case=False)) & (drivers['surname'].str.contains(surname, case=False))].index.values[0]\n",
    "        # create the RDF node for driver\n",
    "        Driver = URIRef(FO[\"driver\"+str(dId)])\n",
    "        # add the edge connecting the Rating and the Driver \n",
    "        g.add((Rating, FO['hasDriver'], Driver))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5dc07d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 31.2 ms\n",
      "Wall time: 96.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'ratings.ttl', 'w', encoding='utf-8') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c823e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
