{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd6904cd",
   "metadata": {},
   "source": [
    "## Populate FASTianF1 RDF database\n",
    "\n",
    "This notebook reports the main steps to download CSV files, process them and create an RDF dataset from them accordingly to an ontology.\n",
    "\n",
    "To measure execution time in Jupyter notebooks: <code>pip install ipython-autotime</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b01ea34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# required libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c483ac7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the required libraries\n",
    "from rdflib import Graph, Literal, RDF, URIRef, Namespace\n",
    "# rdflib knows about some namespaces, like FOAF\n",
    "from rdflib.namespace import FOAF, XSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c483db2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK DATE \n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5fa963",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5543c38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters and URLs\n",
    "path = str(Path(os.path.abspath(os.getcwd())).parent.absolute())\n",
    "\n",
    "circuitsUrl = path + '\\FASTianF1\\data\\DatasetF1\\circuits.csv'\n",
    "constructor_resultsUrl = path + '\\FASTianF1\\data\\DatasetF1\\constructor_results.csv'\n",
    "constructor_standingsUrl = path + '\\FASTianF1\\data\\DatasetF1\\constructor_standings.csv'\n",
    "constructorsUrl = path + '\\FASTianF1\\data\\DatasetF1\\constructors.csv'\n",
    "driver_standingsUrl = path + '\\FASTianF1\\data\\DatasetF1\\driver_standings.csv'\n",
    "driversUrl = path + '\\FASTianF1\\data\\DatasetF1\\drivers.csv'\n",
    "lap_timesUrl = path + '\\FASTianF1\\data\\DatasetF1\\lap_times.csv'\n",
    "pit_stopsUrl = path + '\\FASTianF1\\data\\DatasetF1\\pit_stops.csv'\n",
    "qualifyingUrl = path + '\\FASTianF1\\data\\DatasetF1\\qualifying.csv'\n",
    "racesUrl = path + '\\FASTianF1\\data\\DatasetF1\\\\races.csv'\n",
    "resultsUrl = path + '\\FASTianF1\\data\\DatasetF1\\\\results.csv'\n",
    "sprint_resultsUrl = path + '\\FASTianF1\\data\\DatasetF1\\sprint_results.csv'\n",
    "statusUrl = path + '\\FASTianF1\\data\\DatasetF1\\status.csv'\n",
    "ratingsUrl = path + '\\FASTianF1\\data\\DatasetF1\\\\ratings.csv'\n",
    "seasonsUrl = path + '\\FASTianF1\\data\\DatasetF1\\seasons.csv'\n",
    "\n",
    "# country codes and nationalities conversion\n",
    "countriesURL = path + '\\FASTianF1\\data\\countryCodes\\wikipedia-iso-country-codes.csv'\n",
    "nationalitiesURL = path + '\\FASTianF1\\data\\countryCodes\\\\nationalities.csv'\n",
    "\n",
    "# saving folder\n",
    "savePath =  path + '\\FASTianF1\\data\\\\rdf\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806e71b5",
   "metadata": {},
   "source": [
    "# Namespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf08a3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the country and the movie ontology namespaces not known by RDFlib\n",
    "CNS = Namespace(\"http://eulersharp.sourceforge.net/2003/03swap/countries#\")\n",
    "FO = Namespace(\"https://www.dei.unipd.it/db2/groupProject/FASTianF1#\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7880d54",
   "metadata": {},
   "source": [
    "# Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdf231c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the country codes\n",
    "# we need to convert NaN values to something else otherwise NA strings are converted to NaN -> problem with Namibia\n",
    "countries = pd.read_csv(countriesURL, sep=',', index_col='English short name lower case', keep_default_na=False, na_values=['_'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b8b325",
   "metadata": {},
   "source": [
    "# Drivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93a236e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "drivers = pd.read_csv(driversUrl, sep=',', index_col='driverId')\n",
    "nationalities = pd.read_csv(nationalitiesURL, sep=',', index_col='num_code')\n",
    "# cast year to int. If type(year) = str --> Literal= year-01-01\n",
    "# movies.astype({'year': 'int32'}).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "965c1b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"fo\", FO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "077e008d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 641 ms\n",
      "Wall time: 636 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the drivers dataframe\n",
    "for index, row in drivers.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + \"driver\" + the driver id as URI\n",
    "    Driver = URIRef(FO[\"driver\"+str(index)])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Driver, RDF.type, FO.Driver))\n",
    "    g.add((Driver, FO['hasDriverRef'], Literal(row['driverRef'], datatype=XSD.string)))\n",
    "    if(str(row['number']) != '\\\\N'):\n",
    "        g.add((Driver, FO['hasDriverNumber'], Literal(row['number'], datatype=XSD.integer)))\n",
    "    if(str(row['code']) != '\\\\N'):\n",
    "        g.add((Driver, FO['hasCode'], Literal(row['code'], datatype=XSD.string)))\n",
    "    g.add((Driver, FO['hasForename'], Literal(row['forename'], datatype=XSD.string)))\n",
    "    g.add((Driver, FO['hasSurname'], Literal(row['surname'], datatype=XSD.string)))\n",
    "    g.add((Driver, FO['hasURL'], Literal(row['url'], datatype=XSD.string)))\n",
    "    \n",
    "    #Check that the date has the year-month-day format otherwise print error\n",
    "    try:\n",
    "        datetime.datetime.strptime(str(row['dob']), '%Y-%m-%d')\n",
    "        g.add((Driver, FO['hasDateOfBirth'], Literal(row['dob'], datatype=XSD.date)))\n",
    "    except ValueError:\n",
    "        print(\"Incorrect date format\")\n",
    "\n",
    "    ## handle nationality\n",
    "    # there can be more than one nationality per driver\n",
    "    for nationality in str(row['nationality']).split('-'):\n",
    "        nationalityName = nationality.strip()\n",
    "        # check if the nationality exists in the nationalities dataframe\n",
    "        # str.contains() returns an array of booleans, thus we need to use the any() method\n",
    "        if((nationalities['nationality'].str.contains(nationalityName, case=False)).any() == True):\n",
    "            #get the country code, convert to string and get the lower case to match the country codes in the ontology\n",
    "            #There are multiple countries that correspond to American nationality, then the code for Americans is manually set to \"us\"\n",
    "            if(nationalityName != \"American\"):\n",
    "                code = str(nationalities[nationalities['nationality'].str.contains(nationalityName, case=False)]['alpha_2_code'].values[0]).lower()\n",
    "            else:\n",
    "                code = \"us\"\n",
    "            # create the RDF node for Country\n",
    "            Country = URIRef(CNS[code])\n",
    "            # add the edge connecting the Driver and the Country \n",
    "            g.add((Driver, FO['hasNation'], Country))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ff9af26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 172 ms\n",
      "Wall time: 166 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'drivers.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa815a7",
   "metadata": {},
   "source": [
    "# Circuits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6e32881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "circuits = pd.read_csv(circuitsUrl, sep=',', index_col='circuitId')\n",
    "# cast year to int. If type(year) = str --> Literal= year-01-01\n",
    "# movies.astype({'year': 'int32'}).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "026c7a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"fo\", FO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d99e3dd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 46.9 ms\n",
      "Wall time: 38.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "substitutions = {'ó': 'o', 'ü': 'u', 'ã': 'a', ' ': ''}\n",
    "substitutions2 = {'UAE': 'United Arab Emirates', 'USA': 'United States', 'UK': 'United Kingdom', 'Russia': 'Russian Federation', 'Korea': 'Korea, Republic of'}\n",
    "\n",
    "#iterate over the circuits dataframe\n",
    "for index, row in circuits.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + \"circuits\" + the circuit id as URI\n",
    "    Circuit = URIRef(FO[\"circuit\"+str(index)])\n",
    "    loc = str(row['location'])\n",
    "    # substitution of special characters with standard characters\n",
    "    # special characters are not allowed in URIs\n",
    "    for old, new in substitutions.items():\n",
    "        loc = loc.replace(old, new)\n",
    "    # create the RDF node for location\n",
    "    Location = URIRef(FO[\"location\"+loc])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Location, RDF.type, FO.Location))\n",
    "    g.add((Circuit, RDF.type, FO.Circuit))\n",
    "    g.add((Circuit, FO['hasCircuitRef'], Literal(row['circuitRef'], datatype=XSD.string)))\n",
    "    g.add((Circuit, FO['hasName'], Literal(row['name'], datatype=XSD.string)))\n",
    "    g.add((Circuit, FO['hasLat'], Literal(row['lat'], datatype=XSD.float)))\n",
    "    g.add((Circuit, FO['hasLng'], Literal(row['lng'], datatype=XSD.float)))\n",
    "    if(str(row['alt']) != '\\\\N'):\n",
    "        g.add((Circuit, FO['hasAlt'], Literal(row['alt'], datatype=XSD.float)))\n",
    "    g.add((Circuit, FO['hasURL'], Literal(row['url'], datatype=XSD.string)))\n",
    "    # add the edge connecting the Circuit and the Location \n",
    "    g.add((Circuit, FO['hasLocation'], Location))\n",
    "\n",
    "    ## handle country\n",
    "    countryName = str(row['country'])\n",
    "    # substitution of abbreviations in country names for full names\n",
    "    for old, new in substitutions2.items():\n",
    "        countryName = countryName.replace(old, new)\n",
    "    # check if the country exists\n",
    "    # str.contains() returns an array of booleans, thus we need to use the any() method\n",
    "    if((countries.index == countryName).any() == True):\n",
    "        #get the country code, convert to string and get the lower case to match the country codes in the ontology \n",
    "        code = str(countries[countries.index == countryName]['Alpha-2 code'][0]).lower()\n",
    "        # create the RDF node for Country\n",
    "        Country = URIRef(CNS[code])\n",
    "        # add the edge connecting the Location and the Country \n",
    "        g.add((Location, FO['hasCountry'], Country))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1e8fedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 31.2 ms\n",
      "Wall time: 20.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'circuits.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab133277",
   "metadata": {},
   "source": [
    "# Constructors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6908695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "constructors = pd.read_csv(constructorsUrl, sep=',', index_col='constructorId')\n",
    "# cast year to int. If type(year) = str --> Literal= year-01-01\n",
    "# movies.astype({'year': 'int32'}).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63c69926",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"fo\", FO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8612dfde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 141 ms\n",
      "Wall time: 135 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the constructors dataframe\n",
    "for index, row in constructors.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + \"constructor\" + the constructor id as URI\n",
    "    Constructor = URIRef(FO[\"constructor\"+str(index)])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Constructor, RDF.type, FO.Constructor))\n",
    "    g.add((Constructor, FO['hasConstructorRef'], Literal(row['constructorRef'], datatype=XSD.string)))\n",
    "    g.add((Constructor, FO['hasName'], Literal(row['name'], datatype=XSD.string)))\n",
    "    g.add((Constructor, FO['hasURL'], Literal(row['url'], datatype=XSD.string)))\n",
    "\n",
    "    ## handle nationality\n",
    "    #there can be more than one nationality per constructor\n",
    "    for nationality in str(row['nationality']).split('-'):\n",
    "        nationalityName = nationality.strip()\n",
    "        # check if the nationality exists\n",
    "        # str.contains() returns an array of booleans, thus we need to use the any() method\n",
    "        if((nationalities['nationality'].str.contains(nationalityName, case=False)).any() == True):\n",
    "            #get the country code, convert to string and get the lower case to match the country codes in the ontology \n",
    "            #There are multiple countries that correspond to American nationality, then the code for Americans is manually set to \"us\"\n",
    "            if(nationalityName != \"American\"):\n",
    "                code = str(nationalities[nationalities['nationality'].str.contains(nationalityName, case=False)]['alpha_2_code'].values[0]).lower()\n",
    "            else:\n",
    "                code = \"us\"\n",
    "            # create the RDF node for country\n",
    "            Country = URIRef(CNS[code])\n",
    "            # add the edge connecting the Constructor and the Country \n",
    "            g.add((Constructor, FO['hasNation'], Country))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59913aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 46.9 ms\n",
      "Wall time: 29.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'constructors.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577738ca",
   "metadata": {},
   "source": [
    "# Status and Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29c60ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "status = pd.read_csv(statusUrl, sep=',', index_col='statusId')\n",
    "seasons = pd.read_csv(seasonsUrl, sep=',', index_col='year')\n",
    "# cast year to int. If type(year) = str --> Literal= year-01-01\n",
    "# movies.astype({'year': 'int32'}).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c386e7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 19.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the status dataframe\n",
    "for index, row in status.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + \"status\" + the status id as URI\n",
    "    Status = URIRef(FO[\"status\"+str(index)])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Status, RDF.type, FO.Status))\n",
    "    g.add((Status, FO['hasName'], Literal(row['status'], datatype=XSD.string)))\n",
    "    \n",
    "#iterate over the seasons dataframe\n",
    "for index, row in seasons.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + \"season\" + the season id as URI\n",
    "    Season = URIRef(FO[\"season\"+str(index)])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Season, RDF.type, FO.Season))\n",
    "    g.add((Season, FO['hasYear'], Literal(int(index), datatype=XSD.integer)))\n",
    "    g.add((Season, FO['hasURL'], Literal(row['url'], datatype=XSD.string)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34df99ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 62.5 ms\n",
      "Wall time: 42.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'status_seasons.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dcd70b",
   "metadata": {},
   "source": [
    "# Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67e22c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "races = pd.read_csv(racesUrl, sep=',', index_col='raceId')\n",
    "# cast year to int. If type(year) = str --> Literal= year-01-01\n",
    "# movies.astype({'year': 'int32'}).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "687a35ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"fo\", FO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "340c39af",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 250 ms\n",
      "Wall time: 247 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the races dataframe\n",
    "for index, row in races.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + \"race\" + the race id as URI\n",
    "    Race = URIRef(FO[\"race\"+str(index)])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Race, RDF.type, FO.Race))\n",
    "    g.add((Race, FO['hasRound'], Literal(row['round'], datatype=XSD.integer)))\n",
    "    g.add((Race, FO['hasName'], Literal(row['name'], datatype=XSD.string)))\n",
    "    g.add((Race, FO['hasDate'], Literal(row['date'], datatype=XSD.date)))\n",
    "    if(str(row['time']) != '\\\\N'):\n",
    "        g.add((Race, FO['hasRaceTime'], Literal(row['time'], datatype=XSD.time)))\n",
    "    g.add((Race, FO['hasURL'], Literal(row['url'], datatype=XSD.string)))\n",
    "    \n",
    "    if(str(row['fp1_date']) != '\\\\N'):\n",
    "        g.add((Race, FO['hasFp1Date'], Literal(row['fp1_date'], datatype=XSD.date)))\n",
    "    if(str(row['fp1_time']) != '\\\\N'):\n",
    "        g.add((Race, FO['hasFp1Time'], Literal(row['fp1_time'], datatype=XSD.time)))\n",
    "    if(str(row['fp2_date']) != '\\\\N'):\n",
    "        g.add((Race, FO['hasFp2Date'], Literal(row['fp2_date'], datatype=XSD.date)))\n",
    "    if(str(row['fp2_time']) != '\\\\N'):\n",
    "        g.add((Race, FO['hasFp2Time'], Literal(row['fp2_time'], datatype=XSD.time)))\n",
    "    if(str(row['fp3_date']) != '\\\\N'):\n",
    "        g.add((Race, FO['hasFp3Date'], Literal(row['fp3_date'], datatype=XSD.date)))\n",
    "    if(str(row['fp3_time']) != '\\\\N'):\n",
    "        g.add((Race, FO['hasFp3Time'], Literal(row['fp3_time'], datatype=XSD.time)))\n",
    "    \n",
    "    # create the RDF node for circuit\n",
    "    Circuit = URIRef(FO[\"circuit\"+str(row['circuitId'])])\n",
    "    # add the edge connecting the Race and the Circuit \n",
    "    g.add((Race, FO['hasCircuit'], Circuit))\n",
    "    \n",
    "    # create the RDF node for season\n",
    "    Season = URIRef(FO[\"season\"+str(row['year'])])\n",
    "    # add the edge connecting the Race and the Season \n",
    "    g.add((Race, FO['inSeason'], Season))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "07d2285d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 234 ms\n",
      "Wall time: 217 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'race.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f27d02",
   "metadata": {},
   "source": [
    "# Race (partecipations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e5804573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "r_partecipations = pd.read_csv(resultsUrl, sep=',', index_col='resultId')\n",
    "constructor_results = pd.read_csv(constructor_resultsUrl, sep=',', index_col=\"constructorResultsId\")\n",
    "# join is a dataframe containing the outer join of participation results (r_partecipations) \n",
    "# and driver results (driver_standings)\n",
    "# raceId and driverId are used as join keys\n",
    "join = r_partecipations.merge(constructor_results, how='left', on=['raceId','constructorId'], suffixes=('', 'Constructor')).fillna('\\\\N')\n",
    "laps = pd.read_csv(lap_timesUrl, sep=',')\n",
    "# cast year to int. If type(year) = str --> Literal= year-01-01\n",
    "# movies.astype({'year': 'int32'}).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6cf33c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"fo\", FO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2d47e1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that transforms a time to standard %H:%M:%S.%f format, \n",
    "# adds the zeros and the missing colon to the beginning and end of the string.\n",
    "def time_formatter(splitted_time):\n",
    "    return \"00:00:00\"[0:8-len(splitted_time[0])] + splitted_time[0] + \".\" + splitted_time[1].ljust(3,\"0\")\n",
    "\n",
    "# Function that transforms a gap time to standard %H:%M:%S.%f format, \n",
    "# adds the zeros and the missing colon to the beginning and end of the string.\n",
    "def gap_formatter(gap: str):\n",
    "    splitted_gap = gap.strip().split('.')\n",
    "    if ((':' not in splitted_gap[0]) and (int(splitted_gap[0])>59)):\n",
    "        delta = datetime.timedelta(seconds = int(splitted_gap[0]))\n",
    "        h, mod = divmod(delta.seconds, 3600)\n",
    "        m, s = divmod(mod, 60)\n",
    "        #print(\"{:02d}:{:02d}:{:02d}.{:03d}\".format(h, m, s, int(splitted_gap[1].ljust(3,\"0\"))))\n",
    "        return \"{:02d}:{:02d}:{:02d}.{:03d}\".format(h, m, s, int(splitted_gap[1].ljust(3,\"0\")))\n",
    "    else:\n",
    "        return time_formatter(splitted_gap)\n",
    "\n",
    "# Function that calculates a driver's actual arrival time using the winner's arrival time and the time distance from it.\n",
    "def time_converter(time_gap: str,race: str,pos: str):\n",
    "    \n",
    "    #FIRST_TIME RETRIEVAL\n",
    "    #print(\"FIRST_TIME\", end=\" \")\n",
    "    tmp = join[(join['raceId'] == int(race)) & (join['positionText'] == '1')]\n",
    "    #print(\"Error\") if (tmp.shape[0]!=1) else None\n",
    "    first_splitted = str(tmp.iloc[0]['time']).strip().split('.')\n",
    "    first_time = datetime.datetime.strptime(time_formatter(first_splitted), \"%H:%M:%S.%f\")\n",
    "    \n",
    "    #DELTA RETRIEVAL\n",
    "    #print(\"DELTA\", end=\" \")\n",
    "    splitted_time_gap = time_gap.strip().split('.')\n",
    "    if ':' in (splitted_time_gap[0]):\n",
    "        formatted_time_gap = time_formatter(splitted_time_gap)\n",
    "        (h, m, s) = formatted_time_gap.split('.')[0].split(':')\n",
    "    else:\n",
    "        (h, m, s) = (0,0, splitted_time_gap[0])\n",
    "    delta = datetime.timedelta(hours=int(h), minutes=int(m), seconds=int(s), milliseconds=int(splitted_time_gap[1]))\n",
    "    \n",
    "    '''\n",
    "    print(\"NEW_TIME\")\n",
    "    if(int(race) < 22):\n",
    "        print(\"POSITION-->\",pos,\"FIRST TIME-->\",first_time, end=\" \")\n",
    "        print(\"DELTA-->\",delta,\"NEW TIME-->\",((first_time + delta).strftime(\"%H:%M:%S.%f\"))[:12])\n",
    "    '''\n",
    "    \n",
    "    return first_time + delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "85ae2206",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 31.7 s\n",
      "Wall time: 31.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the join dataframe\n",
    "for index, row in join.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + \"r_partecipation\" + the partecipation id as URI\n",
    "    R_partecipation = URIRef(FO[\"r_partecipation\"+str(index)])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((R_partecipation, RDF.type, FO.RacePartecipation))\n",
    "    if(str(row['number']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasCarNumber'], Literal(row['number'], datatype=XSD.integer)))\n",
    "    if(str(row['grid']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasStartingGridPosition'], Literal(int(row['grid']), datatype=XSD.integer)))\n",
    "    if(str(row['position']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasPosition'], Literal(row['position'], datatype=XSD.integer)))\n",
    "    if(str(row['positionText']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasPositionText'], Literal(row['positionText'], datatype=XSD.string)))\n",
    "    if(str(row['positionOrder']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasPositionOrder'], Literal(int(row['positionOrder']), datatype=XSD.integer)))\n",
    "    if(str(row['points']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasPoints'], Literal(int(row['points']), datatype=XSD.integer)))\n",
    "    if(str(row['laps']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasLaps'], Literal(int(row['laps']), datatype=XSD.integer)))\n",
    "    if((str(row['time']) != '\\\\N') and (str(row['time']) != '+1:10')):\n",
    "        #print(row['raceId'],row['driverId'], end=\"   \")\n",
    "        if row['time'][0] == \"+\":\n",
    "            new_time = time_converter(str(row['time'])[1:], str(row['raceId']),str(row['positionText']))\n",
    "            g.add((R_partecipation, FO['hasResultTime'], Literal(new_time.strftime(\"%H:%M:%S.%f\")[:12], datatype=XSD.time)))\n",
    "            g.add((R_partecipation, FO['hasResultGap'], Literal(gap_formatter(str(row['time'])[1:]), datatype=XSD.time)))\n",
    "        else:\n",
    "            splitted = str(row['time']).strip().split('.')\n",
    "            g.add((R_partecipation, FO['hasResultTime'], Literal(time_formatter(splitted), datatype=XSD.time)))\n",
    "            g.add((R_partecipation, FO['hasResultGap'], Literal(\"00:00:00.000\", datatype=XSD.time)))\n",
    "            #if(int(row['raceId']) < 22):\n",
    "                #print(\"POSITION-->\",str(row['positionText']),\"FIRST TIME-->\",str(row['time']))\n",
    "    if(str(row['milliseconds']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasMillisecondsResultTime'], Literal(row['milliseconds'], datatype=XSD.integer)))\n",
    "    if(str(row['fastestLap']) != '\\\\N'):\n",
    "        # Get the rows of the laps dataframe with the raceId, driverId and lap values matching those in the current row.\n",
    "        tmp = laps[(laps['raceId'] == row['raceId']) & (laps['driverId'] == row['driverId']) & (laps['lap'] == int(row['fastestLap']))]\n",
    "        #iterate over the rows found\n",
    "        for index2, row2 in tmp.iterrows():\n",
    "            # create the RDF node for lap\n",
    "            Lap = URIRef(FO[\"lap\"+str(index2)])\n",
    "            # add the edge connecting the R_partecipation and the Lap \n",
    "            g.add((R_partecipation, FO['hasFastestLap'], Lap))\n",
    "    if(str(row['rank']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasFastestLapRank'], Literal(row['rank'], datatype=XSD.integer)))\n",
    "    if(str(row['fastestLapTime']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasFastestLapTime'], Literal(\"00:00:00.000\"[0:12-len(str(row['fastestLapTime']))] + \n",
    "                                                                 str(row['fastestLapTime']), datatype=XSD.time)))\n",
    "    if(str(row['fastestLapSpeed']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasFastestLapSpeed'], Literal(float(row['fastestLapSpeed']), datatype=XSD.decimal)))\n",
    "    if(str(row['pointsConstructor']) != '\\\\N'):\n",
    "        g.add((R_partecipation, FO['hasConstructorPoints'], Literal(int(row['pointsConstructor']), datatype=XSD.integer)))\n",
    "    \n",
    "    # create the RDF node for driver\n",
    "    Driver = URIRef(FO[\"driver\"+str(row['driverId'])])\n",
    "    # add the edge connecting the Partecipation and the Driver \n",
    "    g.add((R_partecipation, FO['hasDriver'], Driver))\n",
    "    \n",
    "    if(str(row['constructorId']) != '\\\\N'):\n",
    "        # create the RDF node for constructor\n",
    "        Constructor = URIRef(FO[\"constructor\"+str(int(row['constructorId']))])\n",
    "        # add the edge connecting the Partecipation and the Constructor \n",
    "        g.add((R_partecipation, FO['hasConstructor'], Constructor))\n",
    "    \n",
    "    # create the RDF node for race\n",
    "    Race = URIRef(FO[\"race\"+str(row['raceId'])])\n",
    "    # add the edge connecting the Partecipation and the Race \n",
    "    g.add((R_partecipation, FO['partecipatedInRace'], Race))\n",
    "    \n",
    "    if(str(row['statusId']) != '\\\\N'):\n",
    "        # create the RDF node for status\n",
    "        Status = URIRef(FO[\"status\"+str(int(row['statusId']))])\n",
    "        # add the edge connecting the Partecipation and the Status \n",
    "        g.add((R_partecipation, FO['hasStatus'], Status))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "75c2c025",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 9.31 s\n",
      "Wall time: 9.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'race_partecipations.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2233e44",
   "metadata": {},
   "source": [
    "# Qualifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd044a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "q_partecipations = pd.read_csv(qualifyingUrl, sep=',', index_col='qualifyId')\n",
    "# cast year to int. If type(year) = str --> Literal= year-01-01\n",
    "# movies.astype({'year': 'int32'}).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b3a7a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"fo\", FO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "465d6aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 5.61 s\n",
      "Wall time: 5.61 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the q_partecipations dataframe\n",
    "for index, row in q_partecipations.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + \"q_partecipation\" + the qualifying partecipation id as URI\n",
    "    Q_partecipation = URIRef(FO[\"q_partecipation\"+str(index)])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Q_partecipation, RDF.type, FO.QualifPartecipation))\n",
    "    g.add((Q_partecipation, FO['hasCarNumber'], Literal(row['number'], datatype=XSD.integer)))\n",
    "    g.add((Q_partecipation, FO['hasPosition'], Literal(row['position'], datatype=XSD.integer)))\n",
    "    if(str(row['q1']) != '\\\\N'):\n",
    "        g.add((Q_partecipation, FO['hasQ1Time'], Literal(\"00:00:00.000\"[0:12-len(str(row['q1']))] + \n",
    "                                                         str(row['q1']), datatype=XSD.time)))\n",
    "    if(str(row['q2']) != '\\\\N'):\n",
    "        g.add((Q_partecipation, FO['hasQ2Time'], Literal(\"00:00:00.000\"[0:12-len(str(row['q2']))] + \n",
    "                                                         str(row['q2']), datatype=XSD.time)))\n",
    "    if(str(row['q3']) != '\\\\N'):\n",
    "        g.add((Q_partecipation, FO['hasQ3Time'], Literal(\"00:00:00.000\"[0:12-len(str(row['q3']))] + \n",
    "                                                         str(row['q3']), datatype=XSD.time)))\n",
    "\n",
    "    # create the RDF node for driver\n",
    "    Driver = URIRef(FO[\"driver\"+str(row['driverId'])])\n",
    "    # add the edge connecting the qualifyingPartecipation and the Driver \n",
    "    g.add((Q_partecipation, FO['hasDriver'], Driver))\n",
    "    \n",
    "    # create the RDF node for constructor\n",
    "    Constructor = URIRef(FO[\"constructor\"+str(row['constructorId'])])\n",
    "    # add the edge connecting the Partecipation and the Constructor \n",
    "    g.add((Q_partecipation, FO['hasConstructor'], Constructor))\n",
    "    \n",
    "    # create the RDF node for qualifying\n",
    "    Qualifying = URIRef(FO[\"qualifying\"+str(row['raceId'])])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Qualifying, RDF.type, FO.Qualifying))\n",
    "    # add the edge connecting the qualifyingPartecipation and the Qualifying \n",
    "    g.add((Q_partecipation, FO['partecipatedInQualif'], Qualifying))\n",
    "    \n",
    "    # create the RDF node for race\n",
    "    Race = URIRef(FO[\"race\"+str(row['raceId'])])\n",
    "    # add the edge connecting the Race and the Qualifying \n",
    "    g.add((Race, FO['hasA'], Qualifying))\n",
    "    \n",
    "    # Qualifying starting dates and times are stored in the races dataframe,\n",
    "    # then we retrieve them using the matching raceId\n",
    "    Q_date_time = races[races.index == row['raceId']]\n",
    "    if(str(Q_date_time['quali_date'].values[0]) != \"\\\\N\"):\n",
    "        g.add((Qualifying, FO['hasQualiDate'], Literal(Q_date_time['quali_date'].values[0], datatype=XSD.date)))\n",
    "    if(str(Q_date_time['quali_time'].values[0]) != \"\\\\N\"):\n",
    "        g.add((Qualifying, FO['hasQualiTime'], Literal(Q_date_time['quali_time'].values[0], datatype=XSD.time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "56abe1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 2.02 s\n",
      "Wall time: 2.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'qualifying.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4ff574",
   "metadata": {},
   "source": [
    "# Sprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9271534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "s_partecipations = pd.read_csv(sprint_resultsUrl, sep=',', index_col='resultId')\n",
    "# cast year to int. If type(year) = str --> Literal= year-01-01\n",
    "# movies.astype({'year': 'int32'}).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "af17e29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"fo\", FO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f6544469",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 453 ms\n",
      "Wall time: 460 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the s_partecipations dataframe\n",
    "for index, row in s_partecipations.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + \"s_partecipation\" + the sprint partecipation id as URI\n",
    "    S_partecipation = URIRef(FO[\"s_partecipation\"+str(index)])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((S_partecipation, RDF.type, FO.SprintPartecipation))\n",
    "    g.add((S_partecipation, FO['hasCarNumber'], Literal(row['number'], datatype=XSD.integer)))\n",
    "    g.add((S_partecipation, FO['hasStartingGridPosition'], Literal(row['grid'], datatype=XSD.integer)))\n",
    "    if(str(row['position']) != '\\\\N'):\n",
    "        g.add((S_partecipation, FO['hasPosition'], Literal(row['position'], datatype=XSD.integer)))\n",
    "    g.add((S_partecipation, FO['hasPositionText'], Literal(row['positionText'], datatype=XSD.string)))\n",
    "    g.add((S_partecipation, FO['hasPositionOrder'], Literal(row['positionOrder'], datatype=XSD.integer)))\n",
    "    g.add((S_partecipation, FO['hasPoints'], Literal(row['points'], datatype=XSD.integer)))\n",
    "    g.add((S_partecipation, FO['hasLaps'], Literal(row['laps'], datatype=XSD.integer)))\n",
    "    if(str(row['time']) != '\\\\N'):\n",
    "        if row['time'][0] == \"+\":\n",
    "            new_time = time_converter(str(row['time'])[1:], str(row['raceId']),str(row['positionText']))\n",
    "            g.add((S_partecipation, FO['hasResultTime'], Literal(new_time.strftime(\"%H:%M:%S.%f\")[:12], datatype=XSD.time)))\n",
    "            g.add((S_partecipation, FO['hasResultGap'], Literal(gap_formatter(str(row['time'])[1:]), datatype=XSD.time)))\n",
    "        else:\n",
    "            splitted = str(row['time']).strip().split('.')\n",
    "            g.add((S_partecipation, FO['hasResultTime'], Literal(time_formatter(splitted), datatype=XSD.time)))\n",
    "            g.add((S_partecipation, FO['hasResultGap'], Literal(\"00:00:00.000\", datatype=XSD.time)))\n",
    "    if(str(row['milliseconds']) != '\\\\N'):\n",
    "        g.add((S_partecipation, FO['hasMillisecondsResultTime'], Literal(row['milliseconds'], datatype=XSD.integer)))\n",
    "    if(str(row['fastestLap']) != '\\\\N'):\n",
    "        g.add((S_partecipation, FO['hasFastestLap'], Literal(row['fastestLap'], datatype=XSD.integer)))\n",
    "    if(str(row['fastestLapTime']) != '\\\\N'):\n",
    "        g.add((S_partecipation, FO['hasFastestLapTime'], Literal(\"00:00:00.000\"[0:12-len(str(row['fastestLapTime']))] + \n",
    "                                                                 str(row['fastestLapTime']), datatype=XSD.time)))\n",
    "    \n",
    "    # create the RDF node for driver\n",
    "    Driver = URIRef(FO[\"driver\"+str(row['driverId'])])\n",
    "    # add the edge connecting the sprintPartecipation and the Driver\n",
    "    g.add((S_partecipation, FO['hasDriver'], Driver))\n",
    "    \n",
    "    # create the RDF node for constructor\n",
    "    Constructor = URIRef(FO[\"constructor\"+str(row['constructorId'])])\n",
    "    # add the edge connecting the sprintPartecipation and the Constructor\n",
    "    g.add((S_partecipation, FO['hasConstructor'], Constructor))\n",
    "    \n",
    "    # create the RDF node for sprint\n",
    "    Sprint = URIRef(FO[\"sprint\"+str(row['raceId'])])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Sprint, RDF.type, FO.Sprint))\n",
    "    # add the edge connecting the sprintPartecipation and the Sprint \n",
    "    g.add((S_partecipation, FO['partecipatedInSprint'], Sprint))\n",
    "    \n",
    "    # create the RDF node for race\n",
    "    Race = URIRef(FO[\"race\"+str(row['raceId'])])\n",
    "    # add the edge connecting the Race and the Sprint \n",
    "    g.add((Race, FO['hasA'], Sprint))\n",
    "    \n",
    "    # create the RDF node for status\n",
    "    Status = URIRef(FO[\"status\"+str(row['statusId'])])\n",
    "    # add the edge connecting the sprintPartecipation and the Sprint \n",
    "    g.add((S_partecipation, FO['hasStatus'], Status))\n",
    "    \n",
    "    # Sprint starting dates and times are stored in the races dataframe,\n",
    "    # then we retrieve them using the matching raceId\n",
    "    S_date_time = races[races.index == row['raceId']]\n",
    "    if(str(S_date_time['sprint_date'].values[0]) != \"\\\\N\"):\n",
    "        g.add((Sprint, FO['hasSprintDate'], Literal(S_date_time['sprint_date'].values[0], datatype=XSD.date)))\n",
    "    if(str(S_date_time['sprint_time'].values[0]) != \"\\\\N\"):\n",
    "        g.add((Sprint, FO['hasSprintTime'], Literal(S_date_time['sprint_time'].values[0], datatype=XSD.time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "111ccaf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 78.1 ms\n",
      "Wall time: 88.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'sprint.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9183ef64",
   "metadata": {},
   "source": [
    "# Standings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "65f797ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "driver_standings = pd.read_csv(driver_standingsUrl, sep=',', index_col=\"driverStandingsId\")\n",
    "constructor_standings = pd.read_csv(constructor_standingsUrl, sep=',', index_col=\"constructorStandingsId\")\n",
    "# cast year to int. If type(year) = str --> Literal= year-01-01\n",
    "# movies.astype({'year': 'int32'}).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0bf337c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"fo\", FO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "304c9ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 9.75 s\n",
      "Wall time: 9.73 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the driver_standings dataframe\n",
    "for index, row in driver_standings.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + \"standing\" + the standing id as URI\n",
    "    Standing = URIRef(FO[\"d_standing\"+str(index)])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Standing, RDF.type, FO.DriverStanding))\n",
    "    g.add((Standing, FO['hasTotalPoints'], Literal(int(row['points']), datatype=XSD.integer)))\n",
    "    g.add((Standing, FO['hasTotalPosition'], Literal(int(row['position']), datatype=XSD.integer)))\n",
    "    g.add((Standing, FO['hasTotalPositionText'], Literal(row['positionText'], datatype=XSD.string)))\n",
    "    g.add((Standing, FO['hasDriversWins'], Literal(int(row['wins']), datatype=XSD.integer)))\n",
    "\n",
    "    # create the RDF node for driver\n",
    "    Driver = URIRef(FO[\"driver\"+str(row['driverId'])])\n",
    "    # add the edge connecting the Standing and the Driver \n",
    "    g.add((Standing, FO['hasDriver'], Driver))\n",
    "    \n",
    "    # create the RDF node for race\n",
    "    Race = URIRef(FO[\"race\"+str(row['raceId'])])\n",
    "    # add the edge connecting the Standing and the Race \n",
    "    g.add((Standing, FO['hasRace'], Race))\n",
    "    \n",
    "#iterate over the constructor_standings dataframe\n",
    "for index, row in constructor_standings.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + \"standing\" + the standing id as URI\n",
    "    Standing = URIRef(FO[\"c_standing\"+str(index)])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Standing, RDF.type, FO.ConstructorStanding))\n",
    "    g.add((Standing, FO['hasTotalPoints'], Literal(int(row['points']), datatype=XSD.integer)))\n",
    "    g.add((Standing, FO['hasTotalPosition'], Literal(int(row['position']), datatype=XSD.integer)))\n",
    "    g.add((Standing, FO['hasTotalPositionText'], Literal(row['positionText'], datatype=XSD.string)))\n",
    "    g.add((Standing, FO['hasWins'], Literal(int(row['wins']), datatype=XSD.integer)))\n",
    "\n",
    "    # create the RDF node for constructor\n",
    "    Constructor = URIRef(FO[\"constructor\"+str(row['constructorId'])])\n",
    "    # add the edge connecting the Standing and the Constructor\n",
    "    g.add((Standing, FO['hasConstructor'], Constructor))\n",
    "    \n",
    "    # create the RDF node for race\n",
    "    Race = URIRef(FO[\"race\"+str(row['raceId'])])\n",
    "    # add the edge connecting the Standing and the Race \n",
    "    g.add((Standing, FO['hasRace'], Race))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a4155476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 8.72 s\n",
      "Wall time: 8.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'standings.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544125de",
   "metadata": {},
   "source": [
    "# Laps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c504383b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "# Csv already uploaded in Race partecipation section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "283ca1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"fo\", FO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bbb691b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 7min 10s\n",
      "Wall time: 7min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the laps dataframe\n",
    "for index, row in laps.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + \"lap\" + the lap id as URI\n",
    "    Lap = URIRef(FO[\"lap\"+str(index)])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Lap, RDF.type, FO.Lap))\n",
    "    g.add((Lap, FO['hasLapNumber'], Literal(row['lap'], datatype=XSD.integer)))\n",
    "    g.add((Lap, FO['hasLapPosition'], Literal(row['position'], datatype=XSD.integer)))\n",
    "    g.add((Lap, FO['hasLapTime'], Literal(\"00:00:00.000\"[0:12-len(str(row['time']))] + \n",
    "                                          str(row['time']), datatype=XSD.time)))\n",
    "    g.add((Lap, FO['hasMillisecondsTime'], Literal(row['milliseconds'], datatype=XSD.integer)))\n",
    "    \n",
    "    # Get the rows of the join dataframe with the raceId and driverId values matching those in the current row.\n",
    "    tmp = join[(join['raceId'] == row['raceId']) & (join['driverId'] == row['driverId'])]\n",
    "    #iterate over the rows found\n",
    "    for index2, row2 in tmp.iterrows():\n",
    "        # create the RDF node for racePartecipation\n",
    "        R_partecipation = URIRef(FO[\"r_partecipation\"+str(index2)])\n",
    "        # add the edge connecting the racePartecipation and the Lap \n",
    "        g.add((R_partecipation, FO['hasLap'], Lap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4be10ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 1min 33s\n",
      "Wall time: 1min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'laps.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52bc8b3",
   "metadata": {},
   "source": [
    "# Pit stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "190fb428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "pit_stops = pd.read_csv(pit_stopsUrl, sep=',')\n",
    "# cast year to int. If type(year) = str --> Literal= year-01-01\n",
    "# movies.astype({'year': 'int32'}).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4c2ef890",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"fo\", FO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2d447acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 24.3 s\n",
      "Wall time: 24.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the pit_stops dataframe\n",
    "for index, row in pit_stops.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + \"stop\" + the movie id as URI\n",
    "    PitStop = URIRef(FO[\"stop\"+str(index)])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((PitStop, RDF.type, FO.PitStop))\n",
    "    g.add((PitStop, FO['hasStopNumber'], Literal(row['stop'], datatype=XSD.integer)))\n",
    "    g.add((PitStop, FO['hasPitStopTimeOfDay'], Literal(str(row['time']), datatype=XSD.time)))\n",
    "    g.add((PitStop, FO['hasMillisecondsTime'], Literal(row['milliseconds'], datatype=XSD.integer)))\n",
    "    g.add((PitStop, FO['hasDuration'], Literal(\"00:00:00.000\"[0:12-len(str(row['duration']))] + \n",
    "                                               str(row['duration']), datatype=XSD.time)))\n",
    "    \n",
    "    # Get the rows of the laps dataframe with the raceId, driverId and lap values matching those in the current row.\n",
    "    tmp = laps[(laps['raceId'] == row['raceId']) & (laps['driverId'] == row['driverId']) & (laps['lap'] == row['lap'])]\n",
    "    #iterate over the rows found\n",
    "    for index2, row2 in tmp.iterrows():\n",
    "        # create the RDF node for lap\n",
    "        Lap = URIRef(FO[\"lap\"+str(index2)])\n",
    "        # add the edge connecting the PitStop and the Lap \n",
    "        g.add((PitStop, FO['hasPitStopLap'], Lap))\n",
    "        \n",
    "    # Get the rows of the join dataframe with the raceId and driverId values matching those in the current row.\n",
    "    tmp2 = join[(join['raceId'] == row['raceId']) & (join['driverId'] == row['driverId'])]\n",
    "    #iterate over the rows found\n",
    "    for index2, row2 in tmp2.iterrows():\n",
    "        # create the RDF node for racePartecipation\n",
    "        R_partecipation = URIRef(FO[\"r_partecipation\"+str(index2)])\n",
    "        # add the edge connecting the racePartecipation and the PitStop \n",
    "        g.add((R_partecipation, FO['hasPitStop'], PitStop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eaa6d059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 2.03 s\n",
      "Wall time: 2.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'stops.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fdf6c5",
   "metadata": {},
   "source": [
    "# Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8b46dbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "ratings = pd.read_csv(ratingsUrl, sep=',')\n",
    "# cast year to int. If type(year) = str --> Literal= year-01-01\n",
    "# movies.astype({'year': 'int32'}).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "feff869d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"fo\", FO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fad35fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 297 ms\n",
      "Wall time: 300 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the ratings dataframe\n",
    "for index, row in ratings.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    # the node has the namespace + \"rating\" + the movie id as URI\n",
    "    Rating = URIRef(FO[\"rating\"+str(index)])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Rating, RDF.type, FO.Rating))\n",
    "    g.add((Rating, FO['hasPeriod'], Literal(row['Period'], datatype=XSD.date)))\n",
    "    g.add((Rating, FO['hasRating'], Literal(row['Rating'], datatype=XSD.integer)))\n",
    "    g.add((Rating, FO['hasExperience'], Literal(row['Experience'], datatype=XSD.integer)))\n",
    "    g.add((Rating, FO['hasRaceCraft'], Literal(row['Race Craft'], datatype=XSD.integer)))\n",
    "    g.add((Rating, FO['hasAwareness'], Literal(row['Awareness'], datatype=XSD.integer)))\n",
    "    g.add((Rating, FO['hasPace'], Literal(row['Pace'], datatype=XSD.integer)))\n",
    "    g.add((Rating, FO['hasContractCost'], Literal(row['Contract Cost'], datatype=XSD.long)))\n",
    "    g.add((Rating, FO['hasSalary'], Literal(row['Salary'], datatype=XSD.long)))\n",
    "    g.add((Rating, FO['hasBuyout'], Literal(row['Buyout'], datatype=XSD.long)))\n",
    "    \n",
    "    # create the RDF node for season\n",
    "    Season = URIRef(FO[\"season\"+str(row['Year'])])\n",
    "    # add the edge connecting the Rating and the Season \n",
    "    g.add((Rating, FO['inSeason'], Season))\n",
    "    \n",
    "    # in ratings csv names are full-names, then they are splitted in forename and surname\n",
    "    name = str(row['Driver']).split(' ')\n",
    "    forename = name[0].strip()\n",
    "    surname = name[1].strip()\n",
    "    # Get the rows of the drivers dataframe with the forename value (rating csv) contained in forename column of the current row (driver csv).\n",
    "    subCsv = drivers[drivers['forename'].str.contains(forename, case=False)]\n",
    "    # If exists at least one row in subCsv that contain also the surname of the rating\n",
    "    if((subCsv['surname'].str.contains(surname, case=False)).any() == True):\n",
    "        # Get the rows of the drivers dataframe with the forename and surname values contained in the matching column of the current row.\n",
    "        dId = drivers[(drivers['forename'].str.contains(forename, case=False)) & (drivers['surname'].str.contains(surname, case=False))].index.values[0]\n",
    "        # create the RDF node for driver\n",
    "        Driver = URIRef(FO[\"driver\"+str(dId)])\n",
    "        # add the edge connecting the Rating and the Driver \n",
    "        g.add((Rating, FO['hasDriver'], Driver))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5dc07d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 78.1 ms\n",
      "Wall time: 48.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'ratings.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c823e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
